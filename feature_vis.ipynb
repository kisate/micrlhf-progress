{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.gguf import GGUFReader\n",
    "import numpy as np\n",
    "phi = GGUFReader(\"models/phi-3-16.gguf\")\n",
    "abl = GGUFReader(\"models/abl.gguf\")\n",
    "a, (b,), c = phi[\"blk.0.ffn_down.weight\"]\n",
    "_, (b_,), _ = abl[\"blk.0.ffn_down.weight\"]\n",
    "diff = b.reshape(c[::-1]) - b_.reshape(c[::-1])\n",
    "u, s, vt = np.linalg.svd(diff.astype(np.float32), full_matrices=False)\n",
    "vector = u[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"models/phi-3-16.gguf\"\n",
    "from micrlhf.llama import LlamaTransformer\n",
    "llama = LlamaTransformer.from_pretrained(filename, device_map=\"tpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre.{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4787c70a009c406786d45a41aae45684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "from tqdm.auto import trange\n",
    "from penzai.toolshed import sharding_util\n",
    "import random\n",
    "from functools import partial\n",
    "import jax\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(logits, inputs):\n",
    "    losses = pz.nx.nmap(lambda l, i: jnp.take_along_axis(jax.nn.log_softmax(l[:-1], -1), i[1:, None], 1)[:, 0].mean()\n",
    "                        )(logits.untag(\"seq\", \"vocabulary\"), inputs.tokens.untag(\"seq\"))\n",
    "    return -losses\n",
    "\n",
    "bs_start = llama.mesh.shape[\"dp\"]\n",
    "tokens_init = tokenizer.encode(\"<|user|>\\nX X X X X X X X X X X X X X X X X X X X<|end|>\\n<|assistant|>\\n\")\n",
    "optim_mask = [token == 1060 for token in tokens_init]\n",
    "tokens_init = np.asarray(tokens_init)\n",
    "MAX_ELITES = 4\n",
    "tokens_init = np.repeat(tokens_init[None, :], MAX_ELITES, axis=0)\n",
    "seed = 21\n",
    "np.random.seed(seed)\n",
    "tokens_init[:, optim_mask] = np.random.randint(100, tokenizer.vocab_size, tokens_init[:, optim_mask].shape)\n",
    "def tokens_to_array(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    if len(token_array) >= bs_start:\n",
    "        token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\")\n",
    "    return token_array\n",
    "def run_tokens(token_array):\n",
    "    if not isinstance(token_array, pz.nx.NamedArray):\n",
    "        token_array = tokens_to_array(token_array)\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    logits, resids = get_resids_call(inputs)\n",
    "    losses = loss_fn(logits, inputs)\n",
    "    return logits, losses, {resid.tag: resid.value for resid in resids}\n",
    "\n",
    "mask = jax.device_put(jnp.asarray(optim_mask), jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"sp\")))\n",
    "# @partial(jax.jit, static_argnames=(\"key\", \"candidates\", \"expected_changes\"))\n",
    "def algo_iteration(elites, vector, key=\"resid_pre.16\", candidates=1024, seed=13, expected_changes=3):\n",
    "    elites = elites.untag(\"solutions\").tag(\"batch\")\n",
    "    logits = run_tokens(elites)[0].untag(\"batch\").tag(\"elites\")\n",
    "    \n",
    "    def temper(logits, key, elites):\n",
    "        key_choice, key_random = jax.random.split(key)\n",
    "        index = jax.random.randint(key_choice, (), 0, len(logits) - 1)\n",
    "        key_categorical, key_uniform, key_bernoulli, key_randint = jax.random.split(key_random, 4)\n",
    "        logit = logits[index]\n",
    "        elite = elites[index]\n",
    "        logit = jnp.roll(logit, 1, 0)\n",
    "        logit = logit * jax.random.uniform(key_uniform, minval=0.5, maxval=2)\n",
    "        to_change = jax.random.bernoulli(key_bernoulli, expected_changes / mask.sum(), mask.shape)\n",
    "        definite_indices = jax.random.randint(key_randint, mask.shape[:-1], 0, mask.shape[-1])\n",
    "        definite_mask = jax.nn.one_hot(definite_indices, to_change.shape[-1], dtype=jnp.bool_)\n",
    "        to_change = to_change | definite_mask\n",
    "        return jnp.where(mask & to_change,\n",
    "                         jax.random.categorical(key_categorical, logit),\n",
    "                         elite)\n",
    "    tempered_samples = pz.nx.nmap(temper)(\n",
    "        logits.untag(\"elites\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jax.random.split(jax.random.key(seed), candidates), \"batch\"),\n",
    "        elites.untag(\"batch\", \"seq\")).tag(\"seq\")\n",
    "    tempered_samples = sharding_util.name_to_name_device_put(tempered_samples, llama.mesh, dict(batch=\"dp\", seq=\"sp\"))\n",
    "    _, new_losses, new_resids = run_tokens(tempered_samples)\n",
    "    new_scores = (new_resids[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().astype(new_losses.dtype)\n",
    "    metrics = pz.nx.nmap(lambda *xs: jnp.stack(xs))(new_losses, new_scores).tag(\"metrics\")\n",
    "    solution_axes = [k for k in tempered_samples.named_shape.keys() if k != \"seq\"]\n",
    "    solutions = tempered_samples.untag(*solution_axes).flatten().tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    metrics = metrics.untag(*(k for k in solution_axes if k != \"seq\")).flatten().tag(\"solutions\").unwrap(\"solutions\", \"metrics\")\n",
    "    return solutions, metrics\n",
    "\n",
    "\n",
    "best_metrics = None\n",
    "best = tokens_to_array(tokens_init).untag(\"batch\").tag(\"solutions\")\n",
    "xent_min = 1\n",
    "xent_max = 10\n",
    "weights = jnp.stack((\n",
    "    jnp.linspace(-xent_max, -xent_min, MAX_ELITES),\n",
    "    jnp.ones(MAX_ELITES),\n",
    "), -1)\n",
    "for seed in (bar := trange(1_000)):\n",
    "    solutions, metrics = algo_iteration(best, vector, seed=seed)\n",
    "    if best_metrics is not None:\n",
    "        best_metrics = jnp.concatenate((best_metrics, metrics), 0)\n",
    "        best = pz.nx.nmap(lambda a, b: jnp.concatenate((a, b)))(\n",
    "            best.untag(\"solutions\"),\n",
    "            pz.nx.wrap(solutions, \"solutions\", \"seq\").untag(\"solutions\")\n",
    "        ).tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    else:\n",
    "        best_metrics = metrics\n",
    "        best = solutions\n",
    "    elite_mask = (best_metrics[None, :] * weights[:, None]).sum(-1).argmax(1)\n",
    "    best_metrics = best_metrics[elite_mask]\n",
    "    best = pz.nx.wrap(best[elite_mask], \"solutions\", \"seq\")\n",
    "    m = {}\n",
    "    for index in range(MAX_ELITES):\n",
    "        i = index\n",
    "        m |= {f\"decoded.{i}\": tokenizer.decode(best[{\"solutions\": index}].unwrap(\"seq\")),\n",
    "              f\"loss.{i}\": best_metrics[index][0], f\"score.{i}\": best_metrics[index][1]}\n",
    "    bar.set_postfix(**m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, resids = get_resids_call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
