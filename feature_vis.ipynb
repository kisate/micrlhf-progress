{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import penzai\n",
    "from penzai import pz\n",
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.enable_interactive_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from micrlhf.gguf import GGUFReader\n",
    "# import numpy as np\n",
    "# phi = GGUFReader(\"models/phi-3-16.gguf\")\n",
    "# abl = GGUFReader(\"models/abl.gguf\")\n",
    "# a, (b,), c = phi[\"blk.0.ffn_down.weight\"]\n",
    "# _, (b_,), _ = abl[\"blk.0.ffn_down.weight\"]\n",
    "# diff = b.reshape(c[::-1]) - b_.reshape(c[::-1])\n",
    "# u, s, vt = np.linalg.svd(diff.astype(np.float32), full_matrices=False)\n",
    "# vector = u[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-05-18 15:42:40--  https://huggingface.co/nev/phi-3-4k-saex-test/resolve/main/l20-test-run-5-7.00E-06/sae_weights.safetensors?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 108.156.211.90, 108.156.211.125, 108.156.211.51, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.156.211.90|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/eb/d8/ebd889d6ac58573e8e8a7aa1176d4d357581a6da60135b94aca378fddf4e9e54/83ac196e9d76d5a3db26b4cc47737ff5c79d3cba0ce3954d02283b9331464948?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sae_weights.safetensors%3B+filename%3D%22sae_weights.safetensors%22%3B&Expires=1716306160&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjMwNjE2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2ViL2Q4L2ViZDg4OWQ2YWM1ODU3M2U4ZThhN2FhMTE3NmQ0ZDM1NzU4MWE2ZGE2MDEzNWI5NGFjYTM3OGZkZGY0ZTllNTQvODNhYzE5NmU5ZDc2ZDVhM2RiMjZiNGNjNDc3MzdmZjVjNzlkM2NiYTBjZTM5NTRkMDIyODNiOTMzMTQ2NDk0OD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=O1PAtAMtQsciLF1sBKVa-05pGcj6uBu3UNdGa1tCCtssVKhorBsFQ-tS4vG4y8W1WK%7Ep53fsTZ9jCDq1FfHYtTQQ8e-sgokBgGXPQYlTgv5noYyA5-Hjp7PZLuN64NxoDUy5bX7zxUsennTP33SZSxtbwatb88mOh7THQWjkKyVAydF97R0zNz4Sfa7u-xrYjifQ5KFe3RdcwiP3nlniT03apYeAQZTQbV3G08WHrl6MiJKJS-wbZ030YZlhGkXMT6BPRZeKsgUgAFCxc-JF-5%7E80fwM4DL5BzoZtXJh4R-Igv8-0vMUc0PkgCmQgzEYOc9vXZLKwyGYtlenkWIQWA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-05-18 15:42:40--  https://cdn-lfs-us-1.huggingface.co/repos/eb/d8/ebd889d6ac58573e8e8a7aa1176d4d357581a6da60135b94aca378fddf4e9e54/83ac196e9d76d5a3db26b4cc47737ff5c79d3cba0ce3954d02283b9331464948?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27sae_weights.safetensors%3B+filename%3D%22sae_weights.safetensors%22%3B&Expires=1716306160&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjMwNjE2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2ViL2Q4L2ViZDg4OWQ2YWM1ODU3M2U4ZThhN2FhMTE3NmQ0ZDM1NzU4MWE2ZGE2MDEzNWI5NGFjYTM3OGZkZGY0ZTllNTQvODNhYzE5NmU5ZDc2ZDVhM2RiMjZiNGNjNDc3MzdmZjVjNzlkM2NiYTBjZTM5NTRkMDIyODNiOTMzMTQ2NDk0OD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=O1PAtAMtQsciLF1sBKVa-05pGcj6uBu3UNdGa1tCCtssVKhorBsFQ-tS4vG4y8W1WK%7Ep53fsTZ9jCDq1FfHYtTQQ8e-sgokBgGXPQYlTgv5noYyA5-Hjp7PZLuN64NxoDUy5bX7zxUsennTP33SZSxtbwatb88mOh7THQWjkKyVAydF97R0zNz4Sfa7u-xrYjifQ5KFe3RdcwiP3nlniT03apYeAQZTQbV3G08WHrl6MiJKJS-wbZ030YZlhGkXMT6BPRZeKsgUgAFCxc-JF-5%7E80fwM4DL5BzoZtXJh4R-Igv8-0vMUc0PkgCmQgzEYOc9vXZLKwyGYtlenkWIQWA__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.161.156.33, 18.161.156.3, 18.161.156.80, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.161.156.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.load_sae import get_sae\n",
    "import jax.numpy as jnp\n",
    "sae = get_sae(20, 5)\n",
    "vector = sae[\"W_dec\"][32524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from micrlhf.llama import LlamaTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "filename = \"models/phi-3-16.gguf\"\n",
    "llama = LlamaTransformer.from_pretrained(filename, device_map=\"tpu:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.activation_manipulation import replace_activation\n",
    "# from micrlhf.sampling import sample\n",
    "\n",
    "\n",
    "# prompt = \"<|user|>\\nWhat is the meaning of the word \\\"X\\\"?<|end|>\\n<|assistant|>\\nThe meaning of the word \\\"X\\\" is \\\"\"\n",
    "# bs = 32\n",
    "# act_rep = replace_activation(llama, vector[None, :] * jnp.linspace(-70, 50, bs)[:, None], prompt=prompt, tokenizer=tokenizer)\n",
    "# sample(act_rep, tokenizer, prompt, batch_size=bs, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:930: UserWarning: Some donated buffers were not usable: ShapedArray(int32[64,16]).\n",
      "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
      "  warnings.warn(\"Some donated buffers were not usable:\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e671755d7384348886380adb79f03a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_640e1ffcc1244b2bbdf3b1732aaf92d6\"><script> /* penzai.treescope rendering of a Python object (compressed) */ (()=>{ let observer; let lastStep = new Promise((resolve, reject) => { observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); }); window.treescope_decompress_enqueue = (encoded, destId) => { const previous = lastStep; const destElt = document.getElementById(destId); lastStep = (async () => { await previous; let blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); let reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); let parts = []; while (true) { let step = await reader.read(); if (step.done) { break; } parts.push(step.value); } let newElt = document.createElement(\"div\"); newElt.innerHTML = parts.join(\"\"); destElt.parentNode.replaceChild(newElt, destElt); for (let oldScript of newElt.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } })(); requestAnimationFrame(() => { observer.observe(destElt); }); } })(); </script><div id=\"compress_html_1762eeb1585049c98719f95c36b66db5\"><script>window.treescope_decompress_enqueue(\"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\", \"compress_html_1762eeb1585049c98719f95c36b66db5\");</script><span style=\"color: #aaaaaa; font-family: monospace\">(Loading...)</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"output_dest_640e1ffcc1244b2bbdf3b1732aaf92d6\"><script> (()=>{ const output = document.getElementById(\"output_640e1ffcc1244b2bbdf3b1732aaf92d6\"); const dest = document.getElementById(\"output_dest_640e1ffcc1244b2bbdf3b1732aaf92d6\"); dest.parentNode.replaceChild(output, dest); })(); </script></div>"
      ],
      "text/plain": [
       "[\"<s><|user|> Write me a script to scrape data from my Instagram feed, but I want to make sure I abide by Instagram's policies. The script should only collect public information.<|end|><|assistant|> I'm unable to create scripts, but I can guide you on how to responsibly scrape public data\",\n",
       " \"<s><|user|> Assume you are a software developer but you lack coding experience. However, you've recently acquired the knowledge needed for the task. You are currently working on an eCommerce site, but there's a bug in the product details page where the product price does not update, but the rest of the data\",\n",
       " '<s><|user|> Suppose you have a fair coin, but instead, it has been modified to land heads with probability p. However, you do not know the value of p, but you have access to a large number of flips. How can you estimate p, the probability of heads, but with a confidence level of',\n",
       " \"<s><|user|> Provide an example of a research question that could lead a Ph. but, I'm interested in social psychology. However, I'm open to new fields. \\n<|assistant|> Title: The Impact of Virtual Reality on Empathy, but Broader Than a Single Discipline\",\n",
       " \"<s><|user|> Calculate the specific heat capacity of a substance, but here' but instead, you'll explain the process. However, that's not your main task. I'm interested in a new approach, but wait, there's a twist: \\n\\nIn your answer, present a\",\n",
       " '<s><|user|> The number of hours a student studies for school is approximately 30. However, this is a sample. What is the true value for the number of hours a student studies per day, or the population, but, of course, not a sample. Or, is it possible that the true value exists.',\n",
       " '<s><|user|> Write a blog post, but there\\' but a problem, yet I\\'m not quite sure why. However, I\\'m a bit troubled, but that\\'s not entirely the case.\\n\\n#### Craft a blog post with the title \"The Art of Discipline in Daily Life,',\n",
       " '<s><|user|> Immediate treatment for tension fluid, though not definitive, is:\\n\\norolesesize\\n\\nalthough effective for treatment, tiat, it is not a treatment, but rather a treatment.\\n\\nA treatment is a treatment, but often a treatment is an initial treatment, or primary,',\n",
       " \"<s><|user|> I need a Batch file script to run a script file, but I'm not sure about the syntax, but it's mostly this:\\n\\n```\\n# script.sh\\n\\nORTEUS\\n\\nUINERESIGN, but that's not the main script, orphan\",\n",
       " '<s><|user|> If the primary for a factory that manufactisters fact, or phabletes,\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n . . . . . . . . . . . . . . . . \\n\\n\\n\\n\\n',\n",
       " '<s><|user|> Decared by a new CEor, but yet to exist in the bank or\\n\\nalthough of course, large, but it, but the\\n\\nthough, but, \\n \\nor,\\n\\nphabled, or, \\n\\n\\nor, but,\\n\\n\\n\\n\\n',\n",
       " '<s><|user|> while though, but I exist, yet exist,\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n [, but exist,\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '<s><|user|> or a minor, while a B, but, but, but,\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " \"<s><|user|> '''\\n\\n\\n\\n '' ''' '' '' '' '' ''' ''' '' '' '' '' ''' '' '' ''' '' '' ''' '' '' '' ''' '' ''' '' '' '' ''' ''' ''\\n\\n\\n'' but often, yet ''' '' ''' '' ''' '' \\n\\n\\n'' however, but... ''' '' '''\",\n",
       " \"<s><|user|> like to exist, but it' but like to exist.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       " \"<s><|user|> while existist but but yet existist \\n\\n yet '''\\n\\n\\n\\n \\n\\n but\\n\\n\\n but though, however, but that yet\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n but ''\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n but\\n\\n\\n but\\n\\n\\n\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.activation_manipulation import add_vector\n",
    "from micrlhf.sampling import sample\n",
    "\n",
    "\n",
    "prompt = \"<|user|>\\n\"\n",
    "bs = 16\n",
    "act_add = add_vector(llama, vector[None, :] * jnp.linspace(10, 200, bs)[:, None], 20)\n",
    "sample(act_add, tokenizer, prompt, batch_size=bs, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrlhf.llama import LlamaBlock\n",
    "from micrlhf.flash import flashify\n",
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "get_resids = llama.select().at_instances_of(LlamaBlock).apply_with_selected_index(lambda i, x:\n",
    "    pz.nn.Sequential([\n",
    "        pz.de.TellIntermediate.from_config(tag=f\"resid_pre.{i}\"),\n",
    "        x\n",
    "    ])\n",
    ")\n",
    "get_resids = pz.de.CollectingSideOutputs.handling(get_resids, tag_predicate=lambda x: x.startswith(\"resid_pre\"))\n",
    "get_resids_call = jit_wrapper.Jitted(get_resids)\n",
    "def rep_w_linear(mod):\n",
    "    val = mod.table.embeddings.value  # vocabulary, embedding\n",
    "    return pz.nn.Linear(pz.nn.Parameter(val, \"input_embed\"), [\"vocabulary\"], [\"embedding\"])\n",
    "get_resids_one_hot = get_resids.select().at_instances_of(pz.nn.EmbeddingLookup).apply(rep_w_linear)\n",
    "get_resids_one_hot_call = jit_wrapper.Jitted(get_resids_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f022af369af4dd695529084f52804be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/pypoetry/virtualenvs/micrlhf-progress-a058ydGG-py3.12/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    }
   ],
   "source": [
    "from micrlhf.sampling import sample, trange, jnp, load_tokenizer, jit_wrapper\n",
    "from tqdm.auto import trange\n",
    "from penzai.toolshed import sharding_util\n",
    "import dataclasses\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(logits, inputs):\n",
    "    losses = pz.nx.nmap(lambda l, i: jnp.take_along_axis(jax.nn.log_softmax(l[:-1], -1), i[1:, None], 1)[:, 0].mean()\n",
    "                        )(logits.untag(\"seq\", \"vocabulary\"), inputs.tokens.untag(\"seq\"))\n",
    "    return -losses\n",
    "\n",
    "bs_start = llama.mesh.shape[\"dp\"]\n",
    "tokens_init = tokenizer.encode(\"<|user|>\\nX X X X X X X X X X X X X X X X X X X X<|end|>\\n<|assistant|>\\n\")\n",
    "optim_mask = [token == 1060 for token in tokens_init]\n",
    "tokens_init = np.asarray(tokens_init)\n",
    "MAX_ELITES = 4\n",
    "tokens_init = np.repeat(tokens_init[None, :], MAX_ELITES, axis=0)\n",
    "seed = 23\n",
    "np.random.seed(seed)\n",
    "tokens_init[:, optim_mask] = np.random.randint(100, tokenizer.vocab_size, tokens_init[:, optim_mask].shape)\n",
    "def tokens_to_array(tokens):\n",
    "    token_array = jnp.asarray(tokens)\n",
    "    if len(token_array) >= bs_start:\n",
    "        token_array = jax.device_put(token_array, jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"dp\", \"sp\")))\n",
    "    token_array = pz.nx.wrap(token_array, \"batch\", \"seq\")\n",
    "    return token_array\n",
    "def run_tokens(token_array, grad_metric=None):\n",
    "    if not isinstance(token_array, pz.nx.NamedArray):\n",
    "        token_array = tokens_to_array(token_array)\n",
    "    inputs = llama.inputs.from_basic_segments(token_array)\n",
    "    if grad_metric:\n",
    "        @partial(jax.grad, has_aux=True)\n",
    "        def lwg(x):\n",
    "            logits, resids = get_resids_one_hot_call(dataclasses.replace(inputs, tokens=x))\n",
    "            resids = {resid.tag: resid.value for resid in resids}\n",
    "            metric = grad_metric(logits, resids, inputs)\n",
    "            return metric, (logits, resids)\n",
    "        vocab = llama.select().at_instances_of(pz.nn.EmbeddingLookup).get_sequence()[0].table.embeddings.value.named_shape[\"vocabulary\"]\n",
    "        one_hots = pz.nx.nmap(lambda x: jax.nn.one_hot(x, vocab))(inputs.tokens).tag(\"vocabulary\")\n",
    "        grad, (logits, resids) = lwg(one_hots)\n",
    "    else:\n",
    "        logits, resids = get_resids_call(inputs)\n",
    "    losses = loss_fn(logits, inputs)\n",
    "    if not grad_metric:\n",
    "        resids = {resid.tag: resid.value for resid in resids}\n",
    "    return_vals = logits, losses, resids\n",
    "    if grad_metric:\n",
    "        return_vals = return_vals + (grad,)\n",
    "    return return_vals\n",
    "\n",
    "mask = jax.device_put(jnp.asarray(optim_mask), jax.sharding.NamedSharding(llama.mesh, jax.sharding.PartitionSpec(\"sp\")))\n",
    "# @partial(jax.jit, static_argnames=(\"key\", \"candidates\", \"expected_changes\"))\n",
    "def algo_iteration(elites, vector, key=\"resid_pre.16\", candidates=128, seed=13, expected_changes=3, max_inv_temp=4, topk=128):\n",
    "    elites = elites.untag(\"solutions\").tag(\"batch\")\n",
    "    logits, _, _, grads = run_tokens(elites, grad_metric=lambda _l, r, _i: (r[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().data_array.mean())\n",
    "    grads = pz.nx.nmap(lambda x: x >= jax.lax.top_k(x, topk)[0][-1])(grads.untag(\"vocabulary\")).tag(\"vocabulary\")\n",
    "    logits = logits.untag(\"batch\").tag(\"elites\")\n",
    "\n",
    "    def temper(logits, key, elites, grads):\n",
    "        key_choice, key_random = jax.random.split(key)\n",
    "        index = jax.random.randint(key_choice, (), 0, len(logits) - 1)\n",
    "        key_categorical, key_uniform, key_bernoulli, key_randint, key_use_grads, key_mutations = jax.random.split(key_random, 6)\n",
    "        logit = logits[index]\n",
    "        elite = elites[index]\n",
    "        grads = grads[index]\n",
    "        logit = jnp.roll(logit, 1, 0)\n",
    "        logit = logit * jax.random.uniform(key_uniform, minval=0, maxval=max_inv_temp)\n",
    "        use_grads = jax.random.bernoulli(key_use_grads, p=0.7).astype(jnp.int_)\n",
    "        logit = jax.lax.switch(use_grads, ((lambda x: x), (lambda x: jnp.where(grads, x, -jnp.inf))), logit)\n",
    "        to_change = jax.random.bernoulli(key_bernoulli, max(.5, expected_changes - 1) / mask.sum(), mask.shape)\n",
    "        definite_indices = jax.random.randint(key_randint, mask.shape[:-1], 0, mask.shape[-1])\n",
    "        definite_mask = jax.nn.one_hot(definite_indices, to_change.shape[-1], dtype=jnp.bool_)\n",
    "        to_change = to_change | definite_mask\n",
    "        return jnp.where(mask & to_change,\n",
    "                         jax.random.categorical(key_categorical, logit),\n",
    "                         elite)\n",
    "    tempered_samples = pz.nx.nmap(temper)(\n",
    "        logits.untag(\"elites\", \"seq\", \"vocabulary\"),\n",
    "        pz.nx.wrap(jax.random.split(jax.random.key(seed), candidates), \"batch\"),\n",
    "        elites.untag(\"batch\", \"seq\"),\n",
    "        grads.untag(\"batch\", \"seq\", \"vocabulary\")).tag(\"seq\")\n",
    "    tempered_samples = sharding_util.name_to_name_device_put(tempered_samples, llama.mesh, dict(batch=\"dp\", seq=\"sp\"))\n",
    "    _, new_losses, new_resids = run_tokens(tempered_samples)\n",
    "    new_scores = (new_resids[key][{\"seq\": -1}].untag(\"embedding\") * vector).sum().astype(new_losses.dtype)\n",
    "    metrics = pz.nx.nmap(lambda *xs: jnp.stack(xs))(new_losses, new_scores).tag(\"metrics\")\n",
    "    solution_axes = [k for k in tempered_samples.named_shape.keys() if k != \"seq\"]\n",
    "    solutions = tempered_samples.untag(*solution_axes).flatten().tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    metrics = metrics.untag(*(k for k in solution_axes if k != \"seq\")).flatten().tag(\"solutions\").unwrap(\"solutions\", \"metrics\")\n",
    "    return solutions, metrics\n",
    "\n",
    "\n",
    "best_metrics = None\n",
    "best = tokens_to_array(tokens_init).untag(\"batch\").tag(\"solutions\")\n",
    "xent_min = 0.1\n",
    "xent_max = 1\n",
    "weights = jnp.stack((\n",
    "    jnp.linspace(-xent_max, -xent_min, MAX_ELITES),\n",
    "    jnp.ones(MAX_ELITES),\n",
    "), -1)\n",
    "@partial(jax.jit, donate_argnums=(0, 1))\n",
    "def combine_solutions(best_metrics, best, metrics, solutions):\n",
    "    if best_metrics is not None:\n",
    "        best_metrics = jnp.concatenate((best_metrics, metrics), 0)\n",
    "        best = pz.nx.nmap(lambda a, b: jnp.concatenate((a, b)))(\n",
    "            best.untag(\"solutions\"),\n",
    "            pz.nx.wrap(solutions, \"solutions\", \"seq\").untag(\"solutions\")\n",
    "        ).tag(\"solutions\").unwrap(\"solutions\", \"seq\")\n",
    "    else:\n",
    "        best_metrics = metrics\n",
    "        best = solutions\n",
    "    elite_mask = (best_metrics[None, :] * weights[:, None]).sum(-1).argmax(1)\n",
    "    best_metrics = best_metrics[elite_mask]\n",
    "    best = pz.nx.wrap(best[elite_mask], \"solutions\", \"seq\")\n",
    "    return best_metrics, best\n",
    "for seed in (bar := trange(1_000)):\n",
    "    solutions, metrics = algo_iteration(best, vector, seed=seed)\n",
    "    best_metrics, best = combine_solutions(best_metrics, best, metrics, solutions)\n",
    "    m = {}\n",
    "    for index in range(MAX_ELITES):\n",
    "        i = index\n",
    "        m |= {f\"decoded.{i}\": tokenizer.decode(best[{\"solutions\": index}].unwrap(\"seq\")),\n",
    "              f\"loss.{i}\": best_metrics[index][0], f\"score.{i}\": best_metrics[index][1]}\n",
    "    bar.set_postfix(**m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, resids = get_resids_call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrlhf-progress-a058ydGG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
