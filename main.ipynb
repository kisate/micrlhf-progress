{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 0.0 GB out of 128 GB\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "def shmoogle_smi():\n",
    "    jax.profiler.save_device_memory_profile(\"memory.prof\")\n",
    "    pprof_path = \"/usr/local/go/pkg/tool/linux_amd64/pprof\"\n",
    "    out = subprocess.run([pprof_path, \"-top\", \"memory.prof\"], stdout=subprocess.PIPE)\n",
    "    stdout = out.stdout.decode(\"utf-8\")\n",
    "    re_sult = re.search(\n",
    "        r\"Showing nodes accounting for (\\d+(?:\\.\\d+)?)([MG]B)?, (\\d+(?:\\.\\d+)?)% of (\\d+(?:\\.\\d+)?)([MG]B)? total\",\n",
    "        stdout,\n",
    "    )\n",
    "    multiplier = 1 / 1000 if re_sult.group(5) == \"MB\" else 1\n",
    "    total_mem_usage = float(re_sult.group(4))\n",
    "    print(\n",
    "        \"Total mem usage:\",\n",
    "        total_mem_usage * multiplier,\n",
    "        \"GB\",\n",
    "        \"out of\",\n",
    "        16 * len(jax.devices()),\n",
    "        \"GB\",\n",
    "    )\n",
    "\n",
    "\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_with_state' from 'equinox.nn' (/home/neverix/.local/lib/python3.8/site-packages/equinox/nn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama2_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LLaMA\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mequinox\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39meqx\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m~/micrlhf/llama2_model.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjmp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msharding\u001b[39;00m \u001b[39mimport\u001b[39;00m Mesh, NamedSharding, Sharding, PartitionSpec\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstate_util\u001b[39;00m \u001b[39mimport\u001b[39;00m dummy_stateful\n\u001b[1;32m     11\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mWeight\u001b[39;00m(eqx\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     12\u001b[0m     weight: jax\u001b[39m.\u001b[39mArray\n",
      "File \u001b[0;32m~/micrlhf/state_util.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mequinox\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m State, StateIndex, make_with_state\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_state\u001b[39m(args, kwargs):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'make_with_state' from 'equinox.nn' (/home/neverix/.local/lib/python3.8/site-packages/equinox/nn/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "from llama2_model import LLaMA\n",
    "\n",
    "import equinox as eqx\n",
    "import re\n",
    "from safetensors import safe_open\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import jax.numpy as jnp\n",
    "import transformers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15043, 3186, 29991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\"models/Llama-2-7b-hf/tokenizer.model\")\n",
    "input_ids = tokenizer.encode(\"Hello world!\", bos=True, eos=False)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LLaMA...\n",
      "Created LLaMA.\n",
      "Total mem usage: 30.0 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "num_devices = len(jax.devices())\n",
    "mesh = Mesh(np.array(jax.devices()).reshape(-1, 4), axis_names=(\"dp\", \"mp\"))\n",
    "policy = jmp.get_policy(\"p=bf16,c=bf16\")\n",
    "print(\"Creating LLaMA...\")\n",
    "llama = LLaMA(mesh, policy)\n",
    "print(\"Created LLaMA.\")\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "print()\n",
    "for filename in [\n",
    "    \"models/Llama-2-7b-hf/model-00001-of-00002.safetensors\",\n",
    "    \"models/Llama-2-7b-hf/model-00002-of-00002.safetensors\",\n",
    "]:\n",
    "    with safe_open(\n",
    "        filename,\n",
    "        framework=\"numpy\",\n",
    "        device=\"cpu\",\n",
    "    ) as f:\n",
    "        for k in f.keys():\n",
    "            weight = f.get_tensor(k)\n",
    "            if (\n",
    "                k.endswith(\".weight\")\n",
    "                and not k.endswith(\"embed_tokens.weight\")\n",
    "                and not k.endswith(\"norm.weight\")\n",
    "                # and not k.endswith(\"lm_head.weight\")\n",
    "            ):\n",
    "                weight = weight.T\n",
    "            re_sult = re.search(r\"layers\\.([0-9]+)\", k)\n",
    "            try:\n",
    "                k = (\n",
    "                    k[: re_sult.span()[0]]\n",
    "                    + f\"layers[{re_sult.group(1)}]\"\n",
    "                    + k[re_sult.span()[1] :]\n",
    "                )\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            print(\"\\r\" + \" \" * 80, end=\"\")\n",
    "            print(\"\\rLoading\", k, end=\"\")\n",
    "            og = eval(f\"llama.{k}\")\n",
    "            weight = jax.device_put(weight.astype(og.dtype), device=og.sharding)\n",
    "            llama = eval(f\"eqx.tree_at(lambda l: l.{k}, llama, weight)\")\n",
    "print()\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'statify' from 'state_util' (/home/neverix/micrlhf/state_util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstate_util\u001b[39;00m \u001b[39mimport\u001b[39;00m extract_state, statify\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mregister_pytree_node(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         jmp\u001b[39m.\u001b[39mPolicy,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m policy: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m _, tup: jmp\u001b[39m.\u001b[39mPolicy(\u001b[39m*\u001b[39mtup),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/micrlhf/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'statify' from 'state_util' (/home/neverix/micrlhf/state_util.py)"
     ]
    }
   ],
   "source": [
    "from state_util import extract_state, statify\n",
    "\n",
    "\n",
    "try:\n",
    "    jax.tree_util.register_pytree_node(\n",
    "        jmp.Policy,\n",
    "        lambda policy: (\n",
    "            (policy.param_dtype, policy.compute_dtype, policy.output_dtype),\n",
    "            None,\n",
    "        ),\n",
    "        lambda _, tup: jmp.Policy(*tup),\n",
    "    )\n",
    "except ValueError:\n",
    "    pass\n",
    "try:\n",
    "    jax.tree_util.register_pytree_node(\n",
    "        jax._src.numpy.lax_numpy._ScalarMeta, lambda x: (tuple(), x), lambda _, x: x\n",
    "    )\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_multiflat(tree):\n",
    "    if isinstance(tree, (int, float, bool, str, np.ndarray, jax.Array, jmp.Policy)):\n",
    "        return []\n",
    "    leaves, treedef = jax.tree_flatten(tree, is_leaf=lambda x: x is not tree)\n",
    "    if not leaves:\n",
    "        return [tree]\n",
    "    if any(x is tree for x in leaves):\n",
    "        return\n",
    "    leaves = [tree_multiflat(leaf) for leaf in leaves]\n",
    "    return [tree] + leaves\n",
    "\n",
    "\n",
    "# short for multi-level map\n",
    "def tree_mlm(fn, tree):\n",
    "    if not isinstance(tree, eqx.Module):\n",
    "        return []\n",
    "    leaves, treedef = jax.tree_util.tree_flatten_with_path(\n",
    "        tree, is_leaf=lambda x: x is not tree\n",
    "    )\n",
    "    if not leaves:\n",
    "        return tree\n",
    "    if any(x is tree for x in leaves):\n",
    "        return\n",
    "    leaves = [fn(path, tree_mlm(fn, leaf)) for path, leaf in leaves]\n",
    "    return jax.tree_util.tree_unflatten(treedef, leaves)\n",
    "\n",
    "\n",
    "class DebugWrapper(eqx.Module):\n",
    "    module: eqx.Module\n",
    "    index: eqx.nn.StateIndex\n",
    "    path: str\n",
    "\n",
    "    def __init__(self, module, path):\n",
    "        self.module = module\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        _, _, state = extract_state(*args, **kwargs)\n",
    "        output = self.module(*args, **kwargs)\n",
    "        state = state.set(self.index, (args, kwargs, output))\n",
    "        return output, state\n",
    "\n",
    "    def __hasattr__(self, attr):\n",
    "        return hasattr(self.module, attr)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.module, attr)\n",
    "\n",
    "\n",
    "def debugify_model(model):\n",
    "    def new_model(*args, **kwargs):\n",
    "        def debugify(path, leaf):\n",
    "            if not isinstance(leaf, eqx.Module):\n",
    "                return leaf\n",
    "            if not hasattr(leaf, \"__call__\"):\n",
    "                return leaf\n",
    "\n",
    "            path = \"\".join(map(str, path))\n",
    "            return DebugWrapper(leaf, path)\n",
    "\n",
    "        model = statify(tree_mlm(debugify, model))\n",
    "        output, state = model(*args, **kwargs)\n",
    "\n",
    "        debugs = [x for x in tree_multiflat(model) if isinstance(x, DebugWrapper)]\n",
    "        debug = {db.path: state[db.index] for db in debugs}\n",
    "\n",
    "        return output, debug\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "llama_debug = jax.vmap(debugify_model(llama))\n",
    "shmoogle_smi()\n",
    "\n",
    "with mesh:\n",
    "    input_ids = [\n",
    "        tokenizer.encode(x, bos=True, eos=False)\n",
    "        for x in [\"Hello world\", \"This is a test\"]\n",
    "    ]\n",
    "    input_ids = [x + [0] * (128 - len(x)) for x in input_ids]\n",
    "    ids = jnp.asarray(input_ids)\n",
    "    ids = jax.device_put(ids, NamedSharding(mesh, spec=PartitionSpec(\"dp\", None)))\n",
    "    result = llama_debug(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_llama = transformers.LlamaModel.from_pretrained(\"models/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def make_torch_hook(name):\n",
    "    def torch_hook(module, input, output):\n",
    "        print(f\"Module called: {name} {module.__class__}\")\n",
    "        for arg in input:\n",
    "            if isinstance(arg, torch.Tensor):\n",
    "                print(\"Input:\", arg.shape, arg.dtype, str(arg)[:100])\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            print(\"Output:\", output.shape, output.dtype, str(output)[:100])\n",
    "        return output\n",
    "\n",
    "    return torch_hook\n",
    "\n",
    "\n",
    "for name, module in reference_llama.named_modules():\n",
    "    module._forward_hooks.clear()\n",
    "    module.register_forward_hook(make_torch_hook(name))\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "reference_llama(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
