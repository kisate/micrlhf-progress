{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 0.0 GB out of 128 GB\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "def shmoogle_smi():\n",
    "    jax.profiler.save_device_memory_profile(\"memory.prof\")\n",
    "    pprof_path = \"/usr/local/go/pkg/tool/linux_amd64/pprof\"\n",
    "    out = subprocess.run([pprof_path, \"-top\", \"memory.prof\"], stdout=subprocess.PIPE)\n",
    "    stdout = out.stdout.decode(\"utf-8\")\n",
    "    re_sult = re.search(\n",
    "        r\"Showing nodes accounting for (\\d+(?:\\.\\d+)?)([MG]B)?, (\\d+(?:\\.\\d+)?)% of (\\d+(?:\\.\\d+)?)([MG]B)? total\",\n",
    "        stdout,\n",
    "    )\n",
    "    multiplier = 1 / 1000 if re_sult.group(5) == \"MB\" else 1\n",
    "    total_mem_usage = float(re_sult.group(4))\n",
    "    print(\n",
    "        \"Total mem usage:\",\n",
    "        total_mem_usage * multiplier,\n",
    "        \"GB\",\n",
    "        \"out of\",\n",
    "        16 * len(jax.devices()),\n",
    "        \"GB\",\n",
    "    )\n",
    "\n",
    "\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "from llama2_model import LLaMA\n",
    "\n",
    "import equinox as eqx\n",
    "import re\n",
    "from safetensors import safe_open\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import jax.numpy as jnp\n",
    "import transformers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15043, 3186, 29991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\"models/Llama-2-7b-hf/tokenizer.model\")\n",
    "input_ids = tokenizer.encode(\"Hello world!\", bos=True, eos=False)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LLaMA...\n",
      "Created LLaMA.\n",
      "Total mem usage: 29.95 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "num_devices = len(jax.devices())\n",
    "mesh = Mesh(np.array(jax.devices()).reshape(-1, 4), axis_names=(\"dp\", \"mp\"))\n",
    "policy = jmp.get_policy(\"p=bf16,c=bf16\")\n",
    "print(\"Creating LLaMA...\")\n",
    "llama = LLaMA(mesh, policy)\n",
    "print(\"Created LLaMA.\")\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "\n",
      "Loading model.embed_tokens.weight                                               \n",
      "Total mem usage: 33.85 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "print()\n",
    "for filename in [\n",
    "    \"models/Llama-2-7b-hf/model-00001-of-00002.safetensors\",\n",
    "    \"models/Llama-2-7b-hf/model-00002-of-00002.safetensors\",\n",
    "]:\n",
    "    with safe_open(\n",
    "        filename,\n",
    "        framework=\"numpy\",\n",
    "        device=\"cpu\",\n",
    "    ) as f:\n",
    "        for k in [k for k in f.keys() if \"embed\" in k]:  # f.keys():\n",
    "            weight = f.get_tensor(k)\n",
    "            if (\n",
    "                k.endswith(\".weight\")\n",
    "                and not k.endswith(\"embed_tokens.weight\")\n",
    "                and not k.endswith(\"norm.weight\")\n",
    "            ):\n",
    "                weight = weight.T\n",
    "            re_sult = re.search(r\"layers\\.([0-9]+)\", k)\n",
    "            try:\n",
    "                k = (\n",
    "                    k[: re_sult.span()[0]]\n",
    "                    + f\"layers[{re_sult.group(1)}]\"\n",
    "                    + k[re_sult.span()[1] :]\n",
    "                )\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            print(\"\\r\" + \" \" * 80, end=\"\")\n",
    "            print(\"\\rLoading\", k, end=\"\")\n",
    "            og = eval(f\"llama.{k}\")\n",
    "            weight = jax.device_put(weight.astype(og.dtype), device=og.sharding)\n",
    "            llama = eval(f\"eqx.tree_at(lambda l: l.{k}, llama, weight)\")\n",
    "print()\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 34.803580000000004 GB out of 128 GB\n",
      "Module called: .model.embed_tokens (<class 'llama2_model.Embedding'>)\n",
      "Input: () int32 Traced<ShapedArray(int32[])>with<BatchTrace(level=2/0)> with\n",
      "  val = Traced<ShapedArray(int32[128])>\n",
      "Output: ((4096,), dtype('float32')) Traced<ShapedArray(float32[4096])>with<BatchTrace(level=2/0)> with\n",
      "  val = Traced<ShapedArray(float3\n",
      "Module called: .model.layers[0].input_layernorm (<class 'llama2_model.LayerNorm'>)\n",
      "Input: (4096,) bfloat16 Traced<ShapedArray(bfloat16[4096])>with<BatchTrace(level=2/0)> with\n",
      "  val = Traced<ShapedArray(bfloa\n",
      "Output: ((4096,), dtype(bfloat16)) Traced<ShapedArray(bfloat16[4096])>with<BatchTrace(level=2/0)> with\n",
      "  val = Traced<ShapedArray(bfloa\n",
      "Module called: .model.layers[0].self_attn (<class 'llama2_model.SelfAttention'>)\n",
      "Input: (128, 4096) bfloat16 Traced<ShapedArray(bfloat16[128,4096])>with<BatchTrace(level=1/0)> with\n",
      "  val = Array([[[0, -0, 0, .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m ids \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39masarray(input_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m ids \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mdevice_put(ids, NamedSharding(mesh, spec\u001b[39m=\u001b[39mPartitionSpec(\u001b[39m\"\u001b[39m\u001b[39mdp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m result \u001b[39m=\u001b[39m llama_debug(ids)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/api.py:1240\u001b[0m, in \u001b[0;36mvmap.<locals>.vmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m in_axes_flat \u001b[39m=\u001b[39m flatten_axes(\u001b[39m\"\u001b[39m\u001b[39mvmap in_axes\u001b[39m\u001b[39m\"\u001b[39m, in_tree, (in_axes, \u001b[39m0\u001b[39m), kws\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1238\u001b[0m axis_size_ \u001b[39m=\u001b[39m (axis_size \u001b[39mif\u001b[39;00m axis_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m               _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \u001b[39m\"\u001b[39m\u001b[39mvmap\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 1240\u001b[0m out_flat \u001b[39m=\u001b[39m batching\u001b[39m.\u001b[39;49mbatch(\n\u001b[1;32m   1241\u001b[0m     flat_fun, axis_name, axis_size_, in_axes_flat,\n\u001b[1;32m   1242\u001b[0m     \u001b[39mlambda\u001b[39;49;00m: flatten_axes(\u001b[39m\"\u001b[39;49m\u001b[39mvmap out_axes\u001b[39;49m\u001b[39m\"\u001b[39;49m, out_tree(), out_axes),\n\u001b[1;32m   1243\u001b[0m     spmd_axis_name\u001b[39m=\u001b[39;49mspmd_axis_name\n\u001b[1;32m   1244\u001b[0m )\u001b[39m.\u001b[39;49mcall_wrapped(\u001b[39m*\u001b[39;49margs_flat)\n\u001b[1;32m   1245\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(out_tree(), out_flat)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/linear_util.py:188\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m gen \u001b[39m=\u001b[39m gen_static_args \u001b[39m=\u001b[39m out_store \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m   ans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m   \u001b[39m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    191\u001b[0m   \u001b[39m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    192\u001b[0m   \u001b[39m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    193\u001b[0m   \u001b[39m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    194\u001b[0m   \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[39mwhile\u001b[39;00m stack:\n",
      "File \u001b[0;32m~/micrlhf/llama2_model.py:349\u001b[0m, in \u001b[0;36mLLaMA.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39mvmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x))\n",
      "File \u001b[0;32m~/micrlhf/llama2_model.py:320\u001b[0m, in \u001b[0;36mLLaMAModel.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    318\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mcast_to_compute(x)\n\u001b[1;32m    319\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 320\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    321\u001b[0m \u001b[39m# x = jax.lax.scan(lambda carry, layer: layer(carry), x, self.layers)[0]\u001b[39;00m\n\u001b[1;32m    322\u001b[0m x \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm)(x)\n",
      "File \u001b[0;32m~/micrlhf/llama2_model.py:266\u001b[0m, in \u001b[0;36mLLaMALayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 266\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(jax\u001b[39m.\u001b[39;49mvmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm)(x))\n\u001b[1;32m    267\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m jax\u001b[39m.\u001b[39mvmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp)(jax\u001b[39m.\u001b[39mvmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm)(x))\n\u001b[1;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, jax\u001b[39m.\u001b[39mArray):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput:\u001b[39m\u001b[39m\"\u001b[39m, arg\u001b[39m.\u001b[39mshape, arg\u001b[39m.\u001b[39mdtype, \u001b[39mstr\u001b[39m(arg)[:\u001b[39m100\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m jax\u001b[39m.\u001b[39mdebug\u001b[39m.\u001b[39mprint(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOutput: \u001b[39m\u001b[39m{\u001b[39;00m(result\u001b[39m.\u001b[39mshape,\u001b[39m \u001b[39mresult\u001b[39m.\u001b[39mdtype)\u001b[39m \u001b[39m\u001b[39mif\u001b[39;00m\u001b[39m \u001b[39m\u001b[39misinstance\u001b[39m(result,\u001b[39m \u001b[39mjax\u001b[39m.\u001b[39mArray)\u001b[39m \u001b[39m\u001b[39melse\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(result)[:\u001b[39m100\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/micrlhf/llama2_model.py:232\u001b[0m, in \u001b[0;36mSelfAttention.__call__\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m    223\u001b[0m k \u001b[39m=\u001b[39m remb_k(k)\n\u001b[1;32m    224\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    225\u001b[0m     v\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    226\u001b[0m     \u001b[39m+\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q, k, v, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    233\u001b[0m o \u001b[39m=\u001b[39m o\u001b[39m.\u001b[39mreshape(o\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[1;32m    234\u001b[0m o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo_proj(o)\n",
      "File \u001b[0;32m~/micrlhf/llama2_model.py:147\u001b[0m, in \u001b[0;36mAttention.__call__\u001b[0;34m(self, q, k, v, attention_mask)\u001b[0m\n\u001b[1;32m    143\u001b[0m     attention_mask \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mtril(\n\u001b[1;32m    144\u001b[0m         jnp\u001b[39m.\u001b[39mones_like(attention_matrix[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mbool_)\n\u001b[1;32m    145\u001b[0m     )[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    146\u001b[0m attention_matrix \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mwhere(attention_mask, attention_matrix, jnp\u001b[39m.\u001b[39mNINF)\n\u001b[0;32m--> 147\u001b[0m attention_matrix \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49msoftmax(attention_matrix, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    148\u001b[0m o \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39m...abhg,...bhd->...ahgd\u001b[39m\u001b[39m\"\u001b[39m, attention_matrix, v)\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m o\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/nn/functions.py:352\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(x, axis, where, initial)\u001b[0m\n\u001b[1;32m    350\u001b[0m   \u001b[39mreturn\u001b[39;00m _softmax(x, axis, where, initial)\n\u001b[1;32m    351\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m   \u001b[39mreturn\u001b[39;00m _softmax_deprecated(x, axis, where, initial)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/nn/functions.py:376\u001b[0m, in \u001b[0;36m_softmax_deprecated\u001b[0;34m(x, axis, where, initial)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_softmax_deprecated\u001b[39m(x, axis, where, initial):\n\u001b[1;32m    375\u001b[0m   x_max \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmax(x, axis, where\u001b[39m=\u001b[39mwhere, initial\u001b[39m=\u001b[39minitial, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 376\u001b[0m   unnormalized \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mexp(x \u001b[39m-\u001b[39;49m lax\u001b[39m.\u001b[39;49mstop_gradient(x_max))\n\u001b[1;32m    377\u001b[0m   result \u001b[39m=\u001b[39m unnormalized \u001b[39m/\u001b[39m jnp\u001b[39m.\u001b[39msum(unnormalized, axis, where\u001b[39m=\u001b[39mwhere, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    378\u001b[0m   \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/array_methods.py:791\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mop\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 791\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maval, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/array_methods.py:258\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    256\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 258\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m    260\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported operand type(s) for \u001b[39m\u001b[39m{\u001b[39;00mopchar\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:250\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39m@api_boundary\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcache_miss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 250\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[39m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    251\u001b[0m       fun, infer_params_fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    252\u001b[0m   executable \u001b[39m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    253\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:163\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   dispatch\u001b[39m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m   out_flat \u001b[39m=\u001b[39m pjit_p\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs_flat, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:2677\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m axis_main \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((axis_frame(a)\u001b[39m.\u001b[39mmain_trace \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m used_axis_names(\u001b[39mself\u001b[39m, params)),\n\u001b[1;32m   2674\u001b[0m                 default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   2675\u001b[0m top_trace \u001b[39m=\u001b[39m (top_trace \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m axis_main \u001b[39mor\u001b[39;00m axis_main\u001b[39m.\u001b[39mlevel \u001b[39m<\u001b[39m top_trace\u001b[39m.\u001b[39mlevel\n\u001b[1;32m   2676\u001b[0m              \u001b[39melse\u001b[39;00m axis_main\u001b[39m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2677\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/batching.py:398\u001b[0m, in \u001b[0;36mBatchTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    396\u001b[0m   frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_frame(vals_in, dims_in)\n\u001b[1;32m    397\u001b[0m   batched_primitive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_primitive_batcher(primitive, frame)\n\u001b[0;32m--> 398\u001b[0m   val_out, dim_out \u001b[39m=\u001b[39m batched_primitive(vals_in, dims_in, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    399\u001b[0m src \u001b[39m=\u001b[39m source_info_util\u001b[39m.\u001b[39mcurrent()\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m primitive\u001b[39m.\u001b[39mmultiple_results:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1415\u001b[0m, in \u001b[0;36m_pjit_batcher\u001b[0;34m(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)\u001b[0m\n\u001b[1;32m   1407\u001b[0m in_shardings \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m   1408\u001b[0m     _pjit_batcher_for_sharding(i, axis_in, new_parts, mesh, aval\u001b[39m.\u001b[39mndim)\n\u001b[1;32m   1409\u001b[0m     \u001b[39mif\u001b[39;00m axis_in \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m i\n\u001b[1;32m   1410\u001b[0m     \u001b[39mfor\u001b[39;00m axis_in, i, aval \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(dims_in, in_shardings, new_jaxpr\u001b[39m.\u001b[39min_avals))\n\u001b[1;32m   1411\u001b[0m out_shardings \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m   1412\u001b[0m     _pjit_batcher_for_sharding(o, axis_out, new_parts, mesh, aval\u001b[39m.\u001b[39mndim)\n\u001b[1;32m   1413\u001b[0m     \u001b[39mif\u001b[39;00m axis_out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m o\n\u001b[1;32m   1414\u001b[0m     \u001b[39mfor\u001b[39;00m axis_out, o, aval \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(axes_out, out_shardings, new_jaxpr\u001b[39m.\u001b[39mout_avals))\n\u001b[0;32m-> 1415\u001b[0m vals_out \u001b[39m=\u001b[39m pjit_p\u001b[39m.\u001b[39;49mbind(\n\u001b[1;32m   1416\u001b[0m   \u001b[39m*\u001b[39;49mvals_in,\n\u001b[1;32m   1417\u001b[0m   jaxpr\u001b[39m=\u001b[39;49mnew_jaxpr,\n\u001b[1;32m   1418\u001b[0m   in_shardings\u001b[39m=\u001b[39;49min_shardings,\n\u001b[1;32m   1419\u001b[0m   out_shardings\u001b[39m=\u001b[39;49mout_shardings,\n\u001b[1;32m   1420\u001b[0m   resource_env\u001b[39m=\u001b[39;49mresource_env,\n\u001b[1;32m   1421\u001b[0m   donated_invars\u001b[39m=\u001b[39;49mdonated_invars,\n\u001b[1;32m   1422\u001b[0m   name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1423\u001b[0m   keep_unused\u001b[39m=\u001b[39;49mkeep_unused,\n\u001b[1;32m   1424\u001b[0m   inline\u001b[39m=\u001b[39;49minline)\n\u001b[1;32m   1425\u001b[0m \u001b[39mreturn\u001b[39;00m vals_out, axes_out\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:2677\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m axis_main \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((axis_frame(a)\u001b[39m.\u001b[39mmain_trace \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m used_axis_names(\u001b[39mself\u001b[39m, params)),\n\u001b[1;32m   2674\u001b[0m                 default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   2675\u001b[0m top_trace \u001b[39m=\u001b[39m (top_trace \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m axis_main \u001b[39mor\u001b[39;00m axis_main\u001b[39m.\u001b[39mlevel \u001b[39m<\u001b[39m top_trace\u001b[39m.\u001b[39mlevel\n\u001b[1;32m   2676\u001b[0m              \u001b[39melse\u001b[39;00m axis_main\u001b[39m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2677\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1203\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1200\u001b[0m donated_argnums \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(donated_invars) \u001b[39mif\u001b[39;00m d]\n\u001b[1;32m   1201\u001b[0m has_explicit_sharding \u001b[39m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1202\u001b[0m     in_shardings, out_shardings, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1203\u001b[0m \u001b[39mreturn\u001b[39;00m xc\u001b[39m.\u001b[39;49m_xla\u001b[39m.\u001b[39;49mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[1;32m   1204\u001b[0m                     _get_cpp_global_cache(has_explicit_sharding))(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1187\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_impl_cache_miss\u001b[39m(\u001b[39m*\u001b[39margs_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1187\u001b[0m   out_flat, compiled \u001b[39m=\u001b[39m _pjit_call_impl_python(\n\u001b[1;32m   1188\u001b[0m       \u001b[39m*\u001b[39;49margs, jaxpr\u001b[39m=\u001b[39;49mjaxpr, in_shardings\u001b[39m=\u001b[39;49min_shardings,\n\u001b[1;32m   1189\u001b[0m       out_shardings\u001b[39m=\u001b[39;49mout_shardings, resource_env\u001b[39m=\u001b[39;49mresource_env,\n\u001b[1;32m   1190\u001b[0m       donated_invars\u001b[39m=\u001b[39;49mdonated_invars, name\u001b[39m=\u001b[39;49mname, keep_unused\u001b[39m=\u001b[39;49mkeep_unused,\n\u001b[1;32m   1191\u001b[0m       inline\u001b[39m=\u001b[39;49minline)\n\u001b[1;32m   1192\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1193\u001b[0m       compiled, tree_structure(out_flat), args, out_flat)\n\u001b[1;32m   1194\u001b[0m   \u001b[39mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/pjit.py:1120\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mglobal\u001b[39;00m _most_recent_pjit_call_executable\n\u001b[1;32m   1116\u001b[0m in_shardings \u001b[39m=\u001b[39m _resolve_in_shardings(\n\u001b[1;32m   1117\u001b[0m     args, in_shardings, out_shardings,\n\u001b[1;32m   1118\u001b[0m     resource_env\u001b[39m.\u001b[39mphysical_mesh \u001b[39mif\u001b[39;00m resource_env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1120\u001b[0m compiled \u001b[39m=\u001b[39m _pjit_lower(\n\u001b[1;32m   1121\u001b[0m     jaxpr, in_shardings, out_shardings, resource_env,\n\u001b[1;32m   1122\u001b[0m     donated_invars, name, keep_unused, inline,\n\u001b[1;32m   1123\u001b[0m     always_lower\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, lowering_platform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39;49mcompile()\n\u001b[1;32m   1124\u001b[0m _most_recent_pjit_call_executable\u001b[39m.\u001b[39mweak_key_dict[jaxpr] \u001b[39m=\u001b[39m compiled\n\u001b[1;32m   1125\u001b[0m \u001b[39m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2323\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2320\u001b[0m   executable \u001b[39m=\u001b[39m MeshExecutable\u001b[39m.\u001b[39mfrom_trivial_jaxpr(\n\u001b[1;32m   2321\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompile_args)\n\u001b[1;32m   2322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2323\u001b[0m   executable \u001b[39m=\u001b[39m UnloadedMeshExecutable\u001b[39m.\u001b[39;49mfrom_hlo(\n\u001b[1;32m   2324\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2325\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hlo,\n\u001b[1;32m   2326\u001b[0m       \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_args,\n\u001b[1;32m   2327\u001b[0m       compiler_options\u001b[39m=\u001b[39;49mcompiler_options)\n\u001b[1;32m   2328\u001b[0m \u001b[39mif\u001b[39;00m compiler_options \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2329\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executable \u001b[39m=\u001b[39m executable\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2645\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2642\u001b[0m       mesh \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mmesh  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m       \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 2645\u001b[0m xla_executable, compile_options \u001b[39m=\u001b[39m _cached_compilation(\n\u001b[1;32m   2646\u001b[0m     hlo, name, mesh, spmd_lowering,\n\u001b[1;32m   2647\u001b[0m     tuple_args, auto_spmd_lowering, allow_prop_to_outputs,\n\u001b[1;32m   2648\u001b[0m     \u001b[39mtuple\u001b[39;49m(host_callbacks), backend, da, pmap_nreps,\n\u001b[1;32m   2649\u001b[0m     compiler_options_keys, compiler_options_values)\n\u001b[1;32m   2651\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(backend, \u001b[39m\"\u001b[39m\u001b[39mcompile_replicated\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2652\u001b[0m   semantics_in_shardings \u001b[39m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2555\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2550\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, compile_options\n\u001b[1;32m   2552\u001b[0m \u001b[39mwith\u001b[39;00m dispatch\u001b[39m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFinished XLA compilation of \u001b[39m\u001b[39m{fun_name}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{elapsed_time}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2554\u001b[0m     fun_name\u001b[39m=\u001b[39mname, event\u001b[39m=\u001b[39mdispatch\u001b[39m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2555\u001b[0m   xla_executable \u001b[39m=\u001b[39m dispatch\u001b[39m.\u001b[39;49mcompile_or_get_cached(\n\u001b[1;32m   2556\u001b[0m       backend, computation, dev, compile_options, host_callbacks)\n\u001b[1;32m   2557\u001b[0m \u001b[39mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/dispatch.py:497\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    493\u001b[0m use_compilation_cache \u001b[39m=\u001b[39m (compilation_cache\u001b[39m.\u001b[39mis_initialized() \u001b[39mand\u001b[39;00m\n\u001b[1;32m    494\u001b[0m                          backend\u001b[39m.\u001b[39mplatform \u001b[39min\u001b[39;00m supported_platforms)\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_compilation_cache:\n\u001b[0;32m--> 497\u001b[0m   \u001b[39mreturn\u001b[39;00m backend_compile(backend, computation, compile_options,\n\u001b[1;32m    498\u001b[0m                          host_callbacks)\n\u001b[1;32m    500\u001b[0m cache_key \u001b[39m=\u001b[39m compilation_cache\u001b[39m.\u001b[39mget_cache_key(\n\u001b[1;32m    501\u001b[0m     computation, devices, compile_options, backend)\n\u001b[1;32m    503\u001b[0m cached_executable \u001b[39m=\u001b[39m _cache_read(module_name, cache_key, compile_options,\n\u001b[1;32m    504\u001b[0m                                 backend)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama2_model import (\n",
    "    Attention,\n",
    "    RotaryEmbedding,\n",
    "    SelfAttention,\n",
    "    MLP,\n",
    "    LayerNorm,\n",
    "    Embedding,\n",
    ")\n",
    "\n",
    "\n",
    "class DebugWrapper(eqx.Module):\n",
    "    name: str\n",
    "    module: eqx.Module\n",
    "\n",
    "    def __init__(self, name, module):\n",
    "        self.name = name\n",
    "        self.module = module\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        jax.debug.print(f\"Module called: {self.name} ({type(self.module)})\")\n",
    "        for arg in args:\n",
    "            if isinstance(arg, jax.Array):\n",
    "                print(\"Input:\", arg.shape, arg.dtype, str(arg)[:100])\n",
    "        result = self.module(*args, **kwargs)\n",
    "        jax.debug.print(\n",
    "            f\"Output: {(result.shape, result.dtype) if isinstance(result, jax.Array) else ''} {str(result)[:100]}\"\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "debug_type = (SelfAttention, MLP, LayerNorm, Embedding)\n",
    "\n",
    "\n",
    "def is_leaf(mod):\n",
    "    leaf = isinstance(\n",
    "        mod,\n",
    "        debug_type,\n",
    "    )\n",
    "    return leaf\n",
    "\n",
    "\n",
    "def debugify(prefix, x):\n",
    "    if not isinstance(x, debug_type):\n",
    "        return x\n",
    "    return DebugWrapper(\"\".join(map(str, prefix)), x)\n",
    "\n",
    "\n",
    "llama_debug = jax.vmap(\n",
    "    jax.tree_util.tree_map_with_path(\n",
    "        debugify,\n",
    "        llama,\n",
    "        is_leaf=is_leaf,\n",
    "    )\n",
    ")\n",
    "shmoogle_smi()\n",
    "\n",
    "with mesh:\n",
    "    input_ids = [\n",
    "        tokenizer.encode(x, bos=True, eos=False)\n",
    "        for x in [\"Hello world\", \"This is a test\"]\n",
    "    ]\n",
    "    input_ids = [x + [0] * (128 - len(x)) for x in input_ids]\n",
    "    ids = jnp.asarray(input_ids)\n",
    "    ids = jax.device_put(ids, NamedSharding(mesh, spec=PartitionSpec(\"dp\", None)))\n",
    "    result = llama_debug(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "reference_llama = transformers.LlamaModel.from_pretrained(\"models/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module called: embed_tokens <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Input: torch.Size([2, 128]) torch.int64 tensor([[    1, 15043,  3186,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
      "           2.5787e-03, -3.9368e-\n",
      "Module called: layers.0.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
      "           2.5787e-03, -3.9368e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Module called: layers.0.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 1.1561e-01, -4.1347e-01,  7.6590e-01,  ..., -1.1394e-01,\n",
      "           6.2359e-01,  1.6259e-\n",
      "Module called: layers.0.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[-4.5098e-01, -1.6371e-02,  4.9824e-02,  ...,  7.0969e-01,\n",
      "           2.1830e-01,  2.7735e-\n",
      "Module called: layers.0.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[-6.0133e-03, -6.2898e-03,  5.6460e-03,  ...,  1.5145e-03,\n",
      "          -6.9511e-04, -1.4681e-\n",
      "Module called: layers.0.self_attn.rotary_emb <class 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding'>\n",
      "Input: torch.Size([2, 32, 128, 128]) torch.float32 tensor([[[[-6.0133e-03, -6.2898e-03,  5.6460e-03,  ...,  1.4103e-03,\n",
      "            2.0182e-02, -5.3975\n",
      "Output: torch.Size([128, 128]) torch.float32 tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.6479,  0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49347/2092310024.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     module\u001b[39m.\u001b[39mregister_forward_hook(make_torch_hook(name))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(input_ids)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m reference_llama(input_ids)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    923\u001b[0m         hidden_states,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    926\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    673\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    674\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    675\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    676\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    677\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    678\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    679\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:377\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 377\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39mkv_seq_len)\n\u001b[1;32m    378\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def make_torch_hook(name):\n",
    "    def torch_hook(module, input, output):\n",
    "        print(f\"Module called: {name} {module.__class__}\")\n",
    "        for arg in input:\n",
    "            if isinstance(arg, torch.Tensor):\n",
    "                print(\"Input:\", arg.shape, arg.dtype, str(arg)[:100])\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            print(\"Output:\", output.shape, output.dtype, str(output)[:100])\n",
    "        return output\n",
    "\n",
    "    return torch_hook\n",
    "\n",
    "\n",
    "for name, module in reference_llama.named_modules():\n",
    "    module._forward_hooks.clear()\n",
    "    module.register_forward_hook(make_torch_hook(name))\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "reference_llama(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
      "            query_slices = self.q_proj.weight.split(\n",
      "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
      "            )\n",
      "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
      "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
      "\n",
      "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            query_states = torch.cat(query_states, dim=-1)\n",
      "\n",
      "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            key_states = torch.cat(key_states, dim=-1)\n",
      "\n",
      "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            value_states = torch.cat(value_states, dim=-1)\n",
      "\n",
      "        else:\n",
      "            query_states = self.q_proj(hidden_states)\n",
      "            key_states = self.k_proj(hidden_states)\n",
      "            value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # reuse k, v, self_attention\n",
      "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
      "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
      "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
      "        else:\n",
      "            attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(layer.self_attn.forward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
