{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 0.0 GB out of 128 GB\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "def shmoogle_smi():\n",
    "    jax.profiler.save_device_memory_profile(\"memory.prof\")\n",
    "    pprof_path = \"/usr/local/go/pkg/tool/linux_amd64/pprof\"\n",
    "    out = subprocess.run([pprof_path, \"-top\", \"memory.prof\"], stdout=subprocess.PIPE)\n",
    "    stdout = out.stdout.decode(\"utf-8\")\n",
    "    re_sult = re.search(\n",
    "        r\"Showing nodes accounting for (\\d+(?:\\.\\d+)?)([MG]B)?, (\\d+(?:\\.\\d+)?)% of (\\d+(?:\\.\\d+)?)([MG]B)? total\",\n",
    "        stdout,\n",
    "    )\n",
    "    multiplier = 1 / 1000 if re_sult.group(5) == \"MB\" else 1\n",
    "    total_mem_usage = float(re_sult.group(4))\n",
    "    print(\n",
    "        \"Total mem usage:\",\n",
    "        total_mem_usage * multiplier,\n",
    "        \"GB\",\n",
    "        \"out of\",\n",
    "        16 * len(jax.devices()),\n",
    "        \"GB\",\n",
    "    )\n",
    "\n",
    "\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "from llama2_model import LLaMA\n",
    "\n",
    "import equinox as eqx\n",
    "import re\n",
    "from safetensors import safe_open\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import jax.numpy as jnp\n",
    "import transformers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15043, 3186, 29991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\"models/Llama-2-7b-hf/tokenizer.model\")\n",
    "input_ids = tokenizer.encode(\"Hello world!\", bos=True, eos=False)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LLaMA...\n",
      "Created LLaMA.\n",
      "Total mem usage: 29.99 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "num_devices = len(jax.devices())\n",
    "mesh = Mesh(np.array(jax.devices()).reshape(-1, 4), axis_names=(\"dp\", \"mp\"))\n",
    "policy = jmp.get_policy(\"p=bf16,c=bf16\")\n",
    "print(\"Creating LLaMA...\")\n",
    "llama = LLaMA(mesh, policy)\n",
    "print(\"Created LLaMA.\")\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "\n",
      "Loading model.norm.weight                                                       \n",
      "Total mem usage: 29.99 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "print()\n",
    "for filename in [\n",
    "    \"models/Llama-2-7b-hf/model-00001-of-00002.safetensors\",\n",
    "    \"models/Llama-2-7b-hf/model-00002-of-00002.safetensors\",\n",
    "]:\n",
    "    with safe_open(\n",
    "        filename,\n",
    "        framework=\"numpy\",\n",
    "        device=\"cpu\",\n",
    "    ) as f:\n",
    "        for k in f.keys():\n",
    "            weight = f.get_tensor(k)\n",
    "            if (\n",
    "                k.endswith(\".weight\")\n",
    "                and not k.endswith(\"embed_tokens.weight\")\n",
    "                and not k.endswith(\"norm.weight\")\n",
    "                # and not k.endswith(\"lm_head.weight\")\n",
    "            ):\n",
    "                weight = weight.T\n",
    "            re_sult = re.search(r\"layers\\.([0-9]+)\", k)\n",
    "            try:\n",
    "                k = (\n",
    "                    k[: re_sult.span()[0]]\n",
    "                    + f\"layers[{re_sult.group(1)}]\"\n",
    "                    + k[re_sult.span()[1] :]\n",
    "                )\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            print(\"\\r\" + \" \" * 80, end=\"\")\n",
    "            print(\"\\rLoading\", k, end=\"\")\n",
    "            og = eval(f\"llama.{k}\")\n",
    "            weight = jax.device_put(weight.astype(og.dtype), device=og.sharding)\n",
    "            llama = eval(f\"eqx.tree_at(lambda l: l.{k}, llama, weight)\")\n",
    "print()\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 29.99 GB out of 128 GB\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LLaMA.__call__() missing 1 required positional argument: 'cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m ids \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(input_ids)\n\u001b[1;32m    112\u001b[0m ids \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_put(ids, NamedSharding(mesh, spec\u001b[38;5;241m=\u001b[39mPartitionSpec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)))\n\u001b[0;32m--> 113\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllama_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 92\u001b[0m, in \u001b[0;36mdebugify_model.<locals>.new_model\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_model\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 92\u001b[0m     output, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_base\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     debugs \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tree_multiflat(model) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, DebugWrapper)]\n\u001b[1;32m     95\u001b[0m     debug \u001b[38;5;241m=\u001b[39m {db\u001b[38;5;241m.\u001b[39mpath: state\u001b[38;5;241m.\u001b[39mget(db\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;28;01mfor\u001b[39;00m db \u001b[38;5;129;01min\u001b[39;00m debugs}\n",
      "\u001b[0;31mTypeError\u001b[0m: LLaMA.__call__() missing 1 required positional argument: 'cache'"
     ]
    }
   ],
   "source": [
    "from state_util import extract_arg, statify\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "extract_state = partial(extract_arg, arg_name=\"state\")\n",
    "extract_cache = partial(extract_arg, arg_name=\"cache\")\n",
    "\n",
    "\n",
    "try:\n",
    "    jax.tree_util.register_pytree_node(\n",
    "        jmp.Policy,\n",
    "        lambda policy: (\n",
    "            (policy.param_dtype, policy.compute_dtype, policy.output_dtype),\n",
    "            None,\n",
    "        ),\n",
    "        lambda _, tup: jmp.Policy(*tup),\n",
    "    )\n",
    "except ValueError:\n",
    "    pass\n",
    "try:\n",
    "    jax.tree_util.register_pytree_node(\n",
    "        jax._src.numpy.lax_numpy._ScalarMeta, lambda x: (tuple(), x), lambda _, x: x\n",
    "    )\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_multiflat(tree):\n",
    "    if isinstance(tree, (int, float, bool, str, np.ndarray, jax.Array, jmp.Policy)):\n",
    "        return []\n",
    "    leaves, treedef = jax.tree_flatten(tree, is_leaf=lambda x: x is not tree)\n",
    "    if not leaves:\n",
    "        return [tree]\n",
    "    if any(x is tree for x in leaves):\n",
    "        return\n",
    "    leaves = [tree_multiflat(leaf) for leaf in leaves]\n",
    "    return [tree] + leaves\n",
    "\n",
    "\n",
    "# short for multi-level map\n",
    "def tree_mlm(fn, tree):\n",
    "    if not isinstance(tree, eqx.Module):\n",
    "        return tree\n",
    "    leaves, treedef = jax.tree_util.tree_flatten_with_path(\n",
    "        tree, is_leaf=lambda x: x is not tree\n",
    "    )\n",
    "    if not leaves:\n",
    "        return tree\n",
    "    if any(x is tree for x in leaves):\n",
    "        return\n",
    "    leaves = [fn(path, tree_mlm(fn, leaf)) for path, leaf in leaves]\n",
    "    return jax.tree_util.tree_unflatten(treedef, leaves)\n",
    "\n",
    "\n",
    "class DebugWrapper(eqx.Module):\n",
    "    index: eqx.nn.StateIndex\n",
    "    module: eqx.Module\n",
    "    path: str\n",
    "\n",
    "    def __init__(self, module, path):\n",
    "        self.module = module\n",
    "        self.index = eqx.nn.StateIndex(jnp.empty(module.out_shape))\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        _, _, state = extract_state(args, kwargs)\n",
    "        output = self.module(*args, **kwargs)\n",
    "        state = state.set(self.index, (args, kwargs, output))\n",
    "        return output, state\n",
    "\n",
    "    def __hasattr__(self, attr):\n",
    "        return hasattr(self.module, attr)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.module, attr)\n",
    "\n",
    "\n",
    "def debugify_model(model_base):\n",
    "    def debugify(path, leaf):\n",
    "        if not isinstance(leaf, eqx.Module):\n",
    "            return leaf\n",
    "        if not hasattr(leaf, \"__call__\"):\n",
    "            return leaf\n",
    "\n",
    "        path = \"\".join(map(str, path))\n",
    "        return DebugWrapper(leaf, path)\n",
    "\n",
    "    model = tree_mlm(debugify, model_base)\n",
    "    model, state_base = statify(model)\n",
    "    \n",
    "    def new_model(*args, **kwargs):\n",
    "        output, state, cache = model(*args, **{**kwargs, \"state\": state_base, \"cache\": {}})\n",
    "\n",
    "        debugs = [x for x in tree_multiflat(model) if isinstance(x, DebugWrapper)]\n",
    "        debug = {db.path: state.get(db.index) for db in debugs}\n",
    "\n",
    "        return output, debug\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "llama_debug = jax.vmap(debugify_model(llama))\n",
    "shmoogle_smi()\n",
    "\n",
    "with mesh:\n",
    "    input_ids = [\n",
    "        tokenizer.encode(x, bos=True, eos=False)\n",
    "        for x in [\"Hello world\", \"This is a test\"]\n",
    "    ]\n",
    "    input_ids = [x + [0] * (128 - len(x)) for x in input_ids]\n",
    "    ids = jnp.asarray(input_ids)\n",
    "    ids = jax.device_put(ids, NamedSharding(mesh, spec=PartitionSpec(\"dp\", None)))\n",
    "    result = llama_debug(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_llama = transformers.LlamaModel.from_pretrained(\"models/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def make_torch_hook(name):\n",
    "    def torch_hook(module, input, output):\n",
    "        print(f\"Module called: {name} {module.__class__}\")\n",
    "        for arg in input:\n",
    "            if isinstance(arg, torch.Tensor):\n",
    "                print(\"Input:\", arg.shape, arg.dtype, str(arg)[:100])\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            print(\"Output:\", output.shape, output.dtype, str(output)[:100])\n",
    "        return output\n",
    "\n",
    "    return torch_hook\n",
    "\n",
    "\n",
    "for name, module in reference_llama.named_modules():\n",
    "    module._forward_hooks.clear()\n",
    "    module.register_forward_hook(make_torch_hook(name))\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "reference_llama(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
