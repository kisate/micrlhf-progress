{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 0.0 GB out of 128 GB\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "def shmoogle_smi():\n",
    "    jax.profiler.save_device_memory_profile(\"memory.prof\")\n",
    "    pprof_path = \"/usr/local/go/pkg/tool/linux_amd64/pprof\"\n",
    "    out = subprocess.run([pprof_path, \"-top\", \"memory.prof\"], stdout=subprocess.PIPE)\n",
    "    stdout = out.stdout.decode(\"utf-8\")\n",
    "    re_sult = re.search(\n",
    "        r\"Showing nodes accounting for (\\d+(?:\\.\\d+)?)([MG]B)?, (\\d+(?:\\.\\d+)?)% of (\\d+(?:\\.\\d+)?)([MG]B)? total\",\n",
    "        stdout,\n",
    "    )\n",
    "    multiplier = 1 / 1000 if re_sult.group(5) == \"MB\" else 1\n",
    "    total_mem_usage = float(re_sult.group(4))\n",
    "    print(\n",
    "        \"Total mem usage:\",\n",
    "        total_mem_usage * multiplier,\n",
    "        \"GB\",\n",
    "        \"out of\",\n",
    "        16 * len(jax.devices()),\n",
    "        \"GB\",\n",
    "    )\n",
    "\n",
    "\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "from llama2_model import LLaMA\n",
    "\n",
    "import equinox as eqx\n",
    "import re\n",
    "from safetensors import safe_open\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import jax.numpy as jnp\n",
    "import transformers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15043, 3186, 29991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\"models/Llama-2-7b-hf/tokenizer.model\")\n",
    "input_ids = tokenizer.encode(\"Hello world!\", bos=True, eos=False)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LLaMA...\n",
      "Created LLaMA.\n",
      "Total mem usage: 30.0 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "num_devices = len(jax.devices())\n",
    "mesh = Mesh(np.array(jax.devices()).reshape(-1, 4), axis_names=(\"dp\", \"mp\"))\n",
    "policy = jmp.get_policy(\"p=bf16,c=bf16\")\n",
    "print(\"Creating LLaMA...\")\n",
    "llama = LLaMA(mesh, policy)\n",
    "print(\"Created LLaMA.\")\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "\n",
      "Loading model.embed_tokens.weight                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model.norm.weight                                                       \n",
      "Total mem usage: 34.41 GB out of 128 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "print()\n",
    "for filename in [\n",
    "    \"models/Llama-2-7b-hf/model-00001-of-00002.safetensors\",\n",
    "    \"models/Llama-2-7b-hf/model-00002-of-00002.safetensors\",\n",
    "]:\n",
    "    with safe_open(\n",
    "        filename,\n",
    "        framework=\"numpy\",\n",
    "        device=\"cpu\",\n",
    "    ) as f:\n",
    "        for k in f.keys():\n",
    "            weight = f.get_tensor(k)\n",
    "            if (\n",
    "                k.endswith(\".weight\")\n",
    "                and not k.endswith(\"embed_tokens.weight\")\n",
    "                and not k.endswith(\"norm.weight\")\n",
    "                # and not k.endswith(\"lm_head.weight\")\n",
    "            ):\n",
    "                weight = weight.T\n",
    "            re_sult = re.search(r\"layers\\.([0-9]+)\", k)\n",
    "            try:\n",
    "                k = (\n",
    "                    k[: re_sult.span()[0]]\n",
    "                    + f\"layers[{re_sult.group(1)}]\"\n",
    "                    + k[re_sult.span()[1] :]\n",
    "                )\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            print(\"\\r\" + \" \" * 80, end=\"\")\n",
    "            print(\"\\rLoading\", k, end=\"\")\n",
    "            og = eval(f\"llama.{k}\")\n",
    "            weight = jax.device_put(weight.astype(og.dtype), device=og.sharding)\n",
    "            llama = eval(f\"eqx.tree_at(lambda l: l.{k}, llama, weight)\")\n",
    "print()\n",
    "shmoogle_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(body=[If(test=Constant(value=True, kind=None), body=[FunctionDef(name='__call__', args=arguments(posonlyargs=[], args=[arg(arg='self', annotation=None, type_comment=None), arg(arg='x', annotation=None, type_comment=None), arg(arg='attention_mask', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[arg(arg='state', annotation=None, type_comment=None)], kw_defaults=[Dict(keys=[], values=[])], kwarg=None, defaults=[Constant(value=None, kind=None)]), body=[Assign(targets=[Name(id='x', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='policy', ctx=Load()), attr='cast_to_compute', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='policy', ctx=Load()), attr='cast_to_compute', ctx=Load()), args=[Name(id='x', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='policy', ctx=Load()), attr='cast_to_compute', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='q', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='q_proj', ctx=Load())], keywords=[]), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='q_proj', ctx=Load())], keywords=[]), args=[Name(id='x', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='q_proj', ctx=Load())], keywords=[]), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='k', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='k_proj', ctx=Load())], keywords=[]), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='k_proj', ctx=Load())], keywords=[]), args=[Name(id='x', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='k_proj', ctx=Load())], keywords=[]), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='v', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='v_proj', ctx=Load())], keywords=[]), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='v_proj', ctx=Load())], keywords=[]), args=[Name(id='x', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='v_proj', ctx=Load())], keywords=[]), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='remb_k', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='rotary_emb', ctx=Load())], keywords=[keyword(arg='in_axes', value=UnaryOp(op=USub(), operand=Constant(value=2, kind=None))), keyword(arg='out_axes', value=UnaryOp(op=USub(), operand=Constant(value=2, kind=None))), keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='remb_q', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), args=[Name(id='remb_k', ctx=Load())], keywords=[keyword(arg='in_axes', value=UnaryOp(op=USub(), operand=Constant(value=2, kind=None))), keyword(arg='out_axes', value=UnaryOp(op=USub(), operand=Constant(value=2, kind=None))), keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='jax', ctx=Load()), attr='vmap', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='q', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='q', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='q', ctx=Load()), attr='reshape', ctx=Load()), args=[BinOp(left=Subscript(value=Attribute(value=Name(id='q', ctx=Load()), attr='shape', ctx=Load()), slice=Slice(lower=None, upper=UnaryOp(op=USub(), operand=Constant(value=1, kind=None)), step=None), ctx=Load()), op=Add(), right=Tuple(elts=[Attribute(value=Name(id='self', ctx=Load()), attr='num_key_value_heads', ctx=Load()), Attribute(value=Name(id='self', ctx=Load()), attr='_group_size', ctx=Load()), UnaryOp(op=USub(), operand=Constant(value=1, kind=None))], ctx=Load()))], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='q', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='k', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='k', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='k', ctx=Load()), attr='reshape', ctx=Load()), args=[BinOp(left=Subscript(value=Attribute(value=Name(id='k', ctx=Load()), attr='shape', ctx=Load()), slice=Slice(lower=None, upper=UnaryOp(op=USub(), operand=Constant(value=1, kind=None)), step=None), ctx=Load()), op=Add(), right=Tuple(elts=[Attribute(value=Name(id='self', ctx=Load()), attr='num_key_value_heads', ctx=Load()), UnaryOp(op=USub(), operand=Constant(value=1, kind=None))], ctx=Load()))], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='k', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='q', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Name(id='remb_q', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Name(id='remb_q', ctx=Load()), args=[Name(id='q', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Name(id='remb_q', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='k', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Name(id='remb_k', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Name(id='remb_k', ctx=Load()), args=[Name(id='k', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Name(id='remb_k', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='v', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='v', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='v', ctx=Load()), attr='reshape', ctx=Load()), args=[BinOp(left=Subscript(value=Attribute(value=Name(id='v', ctx=Load()), attr='shape', ctx=Load()), slice=Slice(lower=None, upper=UnaryOp(op=USub(), operand=Constant(value=1, kind=None)), step=None), ctx=Load()), op=Add(), right=Tuple(elts=[Attribute(value=Name(id='self', ctx=Load()), attr='num_key_value_heads', ctx=Load()), UnaryOp(op=USub(), operand=Constant(value=1, kind=None))], ctx=Load()))], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='v', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='o', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='attention', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='attention', ctx=Load()), args=[Name(id='q', ctx=Load()), Name(id='k', ctx=Load()), Name(id='v', ctx=Load())], keywords=[keyword(arg='attention_mask', value=Name(id='attention_mask', ctx=Load())), keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='attention', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='o', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='o', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='o', ctx=Load()), attr='reshape', ctx=Load()), args=[BinOp(left=Subscript(value=Attribute(value=Name(id='o', ctx=Load()), attr='shape', ctx=Load()), slice=Slice(lower=None, upper=UnaryOp(op=USub(), operand=Constant(value=3, kind=None)), step=None), ctx=Load()), op=Add(), right=Tuple(elts=[UnaryOp(op=USub(), operand=Constant(value=1, kind=None))], ctx=Load()))], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='o', ctx=Load()), attr='reshape', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Assign(targets=[Name(id='o', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='o_proj', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='o_proj', ctx=Load()), args=[Name(id='o', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Name(id='self', ctx=Load()), attr='o_proj', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Return(value=Tuple(elts=[Name(id='o', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], decorator_list=[], returns=None, type_comment=None)], orelse=[])], type_ignores=[])\n",
      "if True:\n",
      "\n",
      "    def __call__(self, x, attention_mask=None, *, state={}):\n",
      "        x = (state := getattr(self.policy.cast_to_compute, 'store', lambda\n",
      "            state, output: (output, state))(state, self.policy.\n",
      "            cast_to_compute(x, **{'state': state} if hasattr(self.policy.\n",
      "            cast_to_compute, 'store') else {})))[0]\n",
      "        q = (state := getattr(jax.vmap(self.q_proj), 'store', lambda state,\n",
      "            output: (output, state))(state, jax.vmap(self.q_proj)(x, **{\n",
      "            'state': state} if hasattr(jax.vmap(self.q_proj), 'store') else\n",
      "            {})))[0]\n",
      "        k = (state := getattr(jax.vmap(self.k_proj), 'store', lambda state,\n",
      "            output: (output, state))(state, jax.vmap(self.k_proj)(x, **{\n",
      "            'state': state} if hasattr(jax.vmap(self.k_proj), 'store') else\n",
      "            {})))[0]\n",
      "        v = (state := getattr(jax.vmap(self.v_proj), 'store', lambda state,\n",
      "            output: (output, state))(state, jax.vmap(self.v_proj)(x, **{\n",
      "            'state': state} if hasattr(jax.vmap(self.v_proj), 'store') else\n",
      "            {})))[0]\n",
      "        remb_k = (state := getattr(jax.vmap, 'store', lambda state, output:\n",
      "            (output, state))(state, jax.vmap(self.rotary_emb, in_axes=-2,\n",
      "            out_axes=-2, **{'state': state} if hasattr(jax.vmap, 'store') else\n",
      "            {})))[0]\n",
      "        remb_q = (state := getattr(jax.vmap, 'store', lambda state, output:\n",
      "            (output, state))(state, jax.vmap(remb_k, in_axes=-2, out_axes=-\n",
      "            2, **{'state': state} if hasattr(jax.vmap, 'store') else {})))[0]\n",
      "        q = (state := getattr(q.reshape, 'store', lambda state, output: (\n",
      "            output, state))(state, q.reshape(q.shape[:-1] + (self.\n",
      "            num_key_value_heads, self._group_size, -1), **{'state': state} if\n",
      "            hasattr(q.reshape, 'store') else {})))[0]\n",
      "        k = (state := getattr(k.reshape, 'store', lambda state, output: (\n",
      "            output, state))(state, k.reshape(k.shape[:-1] + (self.\n",
      "            num_key_value_heads, -1), **{'state': state} if hasattr(k.\n",
      "            reshape, 'store') else {})))[0]\n",
      "        q = (state := getattr(remb_q, 'store', lambda state, output: (\n",
      "            output, state))(state, remb_q(q, **{'state': state} if hasattr(\n",
      "            remb_q, 'store') else {})))[0]\n",
      "        k = (state := getattr(remb_k, 'store', lambda state, output: (\n",
      "            output, state))(state, remb_k(k, **{'state': state} if hasattr(\n",
      "            remb_k, 'store') else {})))[0]\n",
      "        v = (state := getattr(v.reshape, 'store', lambda state, output: (\n",
      "            output, state))(state, v.reshape(v.shape[:-1] + (self.\n",
      "            num_key_value_heads, -1), **{'state': state} if hasattr(v.\n",
      "            reshape, 'store') else {})))[0]\n",
      "        o = (state := getattr(self.attention, 'store', lambda state, output:\n",
      "            (output, state))(state, self.attention(q, k, v, attention_mask=\n",
      "            attention_mask, **{'state': state} if hasattr(self.attention,\n",
      "            'store') else {})))[0]\n",
      "        o = (state := getattr(o.reshape, 'store', lambda state, output: (\n",
      "            output, state))(state, o.reshape(o.shape[:-3] + (-1,), **{\n",
      "            'state': state} if hasattr(o.reshape, 'store') else {})))[0]\n",
      "        o = (state := getattr(self.o_proj, 'store', lambda state, output: (\n",
      "            output, state))(state, self.o_proj(o, **{'state': state} if\n",
      "            hasattr(self.o_proj, 'store') else {})))[0]\n",
      "        return o, state\n",
      "\n",
      "dict_keys(['__name__', '__loader__', '__spec__', '__package__', '__file__', '__cached__', '__builtins__', 'List', 'eqx', 'jax', 'jnp', 'jmp', 'Mesh', 'NamedSharding', 'Sharding', 'PartitionSpec', 'jdp', 'Weight', 'Embedding', 'LayerNorm', 'MLP', 'RotaryEmbedding', 'Attention', 'SelfAttention', 'LLaMALayer', 'LLaMAModel', 'LLaMA'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function llama2_model.__call__(self, x, attention_mask=None, *, state={})>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama2_model import SelfAttention\n",
    "from ast import (\n",
    "    NodeTransformer,\n",
    "    Expr,\n",
    "    Return,\n",
    "    Subscript,\n",
    "    NamedExpr,\n",
    "    Name,\n",
    "    Store,\n",
    "    Load,\n",
    "    Constant,\n",
    "    Index,\n",
    "    FunctionDef,\n",
    "    Call,\n",
    "    Attribute,\n",
    "    arg,\n",
    "    arguments,\n",
    "    Assign,\n",
    "    Tuple,\n",
    "    Dict,\n",
    "    keyword,\n",
    "    IfExp,\n",
    "    Lambda,\n",
    ")\n",
    "import inspect\n",
    "import ast\n",
    "import dis\n",
    "\n",
    "\n",
    "class ReplaceFuncCall(NodeTransformer):\n",
    "    def visit_Call(self, node):\n",
    "        func = node.func\n",
    "        store = Call(\n",
    "            func=Name(id=\"getattr\", ctx=Load()),\n",
    "            args=[\n",
    "                func,\n",
    "                Constant(value=\"store\", kind=None),\n",
    "                Lambda(\n",
    "                    args=arguments(\n",
    "                        posonlyargs=[],\n",
    "                        args=[\n",
    "                            arg(arg=\"state\", annotation=None, type_comment=None),\n",
    "                            arg(arg=\"output\", annotation=None, type_comment=None),\n",
    "                        ],\n",
    "                        vararg=None,\n",
    "                        kwonlyargs=[],\n",
    "                        kw_defaults=[],\n",
    "                        kwarg=None,\n",
    "                        defaults=[],\n",
    "                    ),\n",
    "                    body=Tuple(\n",
    "                        elts=[\n",
    "                            Name(id=\"output\", ctx=Load()),\n",
    "                            Name(id=\"state\", ctx=Load()),\n",
    "                        ],\n",
    "                        ctx=Load(),\n",
    "                    ),\n",
    "                ),\n",
    "            ],\n",
    "            keywords=[],\n",
    "        )\n",
    "        return Subscript(\n",
    "            value=NamedExpr(\n",
    "                target=Name(id=\"state\", ctx=Store()),\n",
    "                value=Call(\n",
    "                    func=store,\n",
    "                    args=[\n",
    "                        Name(id=\"state\", ctx=Load()),\n",
    "                        Call(\n",
    "                            func=func,\n",
    "                            args=[self.visit(a) for a in node.args],\n",
    "                            keywords=[self.visit(k) for k in node.keywords]\n",
    "                            + [\n",
    "                                keyword(\n",
    "                                    arg=None,\n",
    "                                    value=IfExp(\n",
    "                                        test=Call(\n",
    "                                            func=Name(id=\"hasattr\", ctx=Load()),\n",
    "                                            args=[\n",
    "                                                func,\n",
    "                                                Constant(value=\"store\", kind=None),\n",
    "                                            ],\n",
    "                                            keywords=[],\n",
    "                                        ),\n",
    "                                        body=Dict(\n",
    "                                            keys=[Constant(value=\"state\", kind=None)],\n",
    "                                            values=[Name(id=\"state\", ctx=Load())],\n",
    "                                        ),\n",
    "                                        orelse=Dict(keys=[], values=[]),\n",
    "                                    ),\n",
    "                                )\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                    keywords=[],\n",
    "                ),\n",
    "            ),\n",
    "            slice=Index(value=Constant(value=0, kind=None)),\n",
    "            ctx=Load(),\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        return FunctionDef(\n",
    "            name=node.name,\n",
    "            args=arguments(\n",
    "                args=node.args.args,\n",
    "                vararg=node.args.vararg,\n",
    "                kwonlyargs=node.args.kwonlyargs\n",
    "                + [arg(arg=\"state\", annotation=None, type_comment=None)],\n",
    "                kw_defaults=node.args.kw_defaults + [Dict(keys=[], values=[])],\n",
    "                kwarg=node.args.kwarg,\n",
    "                defaults=node.args.defaults,\n",
    "                posonlyargs=node.args.posonlyargs,\n",
    "            ),\n",
    "            body=\n",
    "            # [\n",
    "            #     Assign(\n",
    "            #         targets=[Name(id=\"state\", ctx=Store())],\n",
    "            #         value=Tuple(\n",
    "            #             elts=[\n",
    "            #                 Constant(value=None, kind=None),\n",
    "            #                 Dict(keys=[], values=[]),\n",
    "            #             ],\n",
    "            #             ctx=Load(),\n",
    "            #         ),\n",
    "            #         type_comment=None,\n",
    "            #     )\n",
    "            # ]\n",
    "            # +\n",
    "            [self.visit(b) for b in node.body],\n",
    "            decorator_list=node.decorator_list,\n",
    "            returns=node.returns,\n",
    "            type_comment=node.type_comment,\n",
    "        )\n",
    "\n",
    "    def visit_Return(self, node: Return):\n",
    "        return Return(\n",
    "            value=Tuple(\n",
    "                elts=[self.visit(node.value), Name(id=\"state\", ctx=Load())],\n",
    "                ctx=Load(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def add_state(func):\n",
    "    source = inspect.getsource(func)\n",
    "    if source.startswith(\" \"):\n",
    "        source = \"if True:\\n\" + source\n",
    "    func_ast = ast.parse(source)\n",
    "    func_ast = ReplaceFuncCall().visit(func_ast)\n",
    "    func_ast = ast.fix_missing_locations(func_ast)\n",
    "    print(ast.dump(func_ast))\n",
    "    code = compile(func_ast, filename=func.__code__.co_filename, mode=\"exec\")\n",
    "    env = func.__globals__.copy()\n",
    "    import astor\n",
    "\n",
    "    print(astor.to_source(func_ast))\n",
    "    print(env.keys())\n",
    "    exec(code, env)\n",
    "    return env[func.__name__]\n",
    "\n",
    "\n",
    "def blabla(a, b):\n",
    "    return a - b\n",
    "\n",
    "\n",
    "blabla.store = lambda state, output: (output, {\"a\": 1, **state})\n",
    "add_state(SelfAttention.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Module(body=[Expr(value=Call(func=Name(id='f', ctx=Load()), args=[Name(id='a', ctx=Load())], keywords=[keyword(arg='b', value=Constant(value=2, kind=None)), keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Name(id='f', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))]))], type_ignores=[])\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.dump(ast.parse(\"f(a, b=2, **({'state': state} if hasattr(f, 'store') else {}))\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mem usage: 30.0 GB out of 128 GB\n",
      "Module(body=[If(test=Constant(value=True, kind=None), body=[FunctionDef(name='__call__', args=arguments(posonlyargs=[], args=[arg(arg='self', annotation=None, type_comment=None), arg(arg='x', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[arg(arg='state', annotation=None, type_comment=None)], kw_defaults=[Dict(keys=[], values=[])], kwarg=None, defaults=[]), body=[Assign(targets=[Name(id='x', ctx=Store())], value=Subscript(value=NamedExpr(target=Name(id='state', ctx=Store()), value=Call(func=Call(func=Name(id='getattr', ctx=Load()), args=[Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='policy', ctx=Load()), attr='cast_to_compute', ctx=Load()), Constant(value='store', kind=None), Lambda(args=arguments(posonlyargs=[], args=[arg(arg='state', annotation=None, type_comment=None), arg(arg='output', annotation=None, type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=Tuple(elts=[Name(id='output', ctx=Load()), Name(id='state', ctx=Load())], ctx=Load()))], keywords=[]), args=[Name(id='state', ctx=Load()), Call(func=Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='policy', ctx=Load()), attr='cast_to_compute', ctx=Load()), args=[Name(id='x', ctx=Load())], keywords=[keyword(arg=None, value=IfExp(test=Call(func=Name(id='hasattr', ctx=Load()), args=[Attribute(value=Attribute(value=Name(id='self', ctx=Load()), attr='policy', ctx=Load()), attr='cast_to_compute', ctx=Load()), Constant(value='store', kind=None)], keywords=[]), body=Dict(keys=[Constant(value='state', kind=None)], values=[Name(id='state', ctx=Load())]), orelse=Dict(keys=[], values=[])))])], keywords=[])), slice=Index(value=Constant(value=0, kind=None)), ctx=Load()), type_comment=None), Return(value=Tuple(elts=[BinOp(left=Name(id='x', ctx=Load()), op=MatMult(), right=Attribute(value=Name(id='self', ctx=Load()), attr='weight', ctx=Load())), Name(id='state', ctx=Load())], ctx=Load()))], decorator_list=[], returns=None, type_comment=None)], orelse=[])], type_ignores=[])\n",
      "if True:\n",
      "\n",
      "    def __call__(self, x, *, state={}):\n",
      "        x = (state := getattr(self.policy.cast_to_compute, 'store', lambda\n",
      "            state, output: (output, state))(state, self.policy.\n",
      "            cast_to_compute(x, **{'state': state} if hasattr(self.policy.\n",
      "            cast_to_compute, 'store') else {})))[0]\n",
      "        return x @ self.weight, state\n",
      "\n",
      "dict_keys(['__name__', '__loader__', '__spec__', '__package__', '__file__', '__cached__', '__builtins__', 'List', 'eqx', 'jax', 'jnp', 'jmp', 'Mesh', 'NamedSharding', 'Sharding', 'PartitionSpec', 'jdp', 'Weight', 'Embedding', 'LayerNorm', 'MLP', 'RotaryEmbedding', 'Attention', 'SelfAttention', 'LLaMALayer', 'LLaMAModel', 'LLaMA'])\n",
      "Weight(\n",
      "  weight=bf16[4096,32000],\n",
      "  policy=Policy(\n",
      "    param_dtype=<class 'jax.numpy.bfloat16'>,\n",
      "    compute_dtype=<class 'jax.numpy.bfloat16'>,\n",
      "    output_dtype=<class 'jax.numpy.bfloat16'>\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Main binary filename not available.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m ids \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39masarray(input_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m ids \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mdevice_put(ids, NamedSharding(mesh, spec\u001b[39m=\u001b[39mPartitionSpec(\u001b[39m\"\u001b[39m\u001b[39mdp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m result \u001b[39m=\u001b[39m llama_debug(ids)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mprint\u001b[39m(leaf)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m         eqx\u001b[39m.\u001b[39mtree_at(\u001b[39mlambda\u001b[39;00m l: l\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m, leaf, call)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tree_mlm(debugify, model)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), \u001b[39mdict\u001b[39m(logs)\n",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(x \u001b[39mis\u001b[39;00m tree \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m leaves):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m leaves \u001b[39m=\u001b[39m [fn(path, tree_mlm(fn, leaf)) \u001b[39mfor\u001b[39;00m path, leaf \u001b[39min\u001b[39;00m leaves]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_unflatten(treedef, leaves)\n",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(x \u001b[39mis\u001b[39;00m tree \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m leaves):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m leaves \u001b[39m=\u001b[39m [fn(path, tree_mlm(fn, leaf)) \u001b[39mfor\u001b[39;00m path, leaf \u001b[39min\u001b[39;00m leaves]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_unflatten(treedef, leaves)\n",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m call\u001b[39m.\u001b[39mstore \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m state, output: (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     output,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, path)): output, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstate},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(leaf)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m eqx\u001b[39m.\u001b[39;49mtree_at(\u001b[39mlambda\u001b[39;49;00m l: l\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m, leaf, call)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/equinox/_tree.py:137\u001b[0m, in \u001b[0;36mtree_at\u001b[0;34m(where, pytree, replace, replace_fn, is_leaf)\u001b[0m\n\u001b[1;32m    135\u001b[0m leaves1, structure1 \u001b[39m=\u001b[39m jtu\u001b[39m.\u001b[39mtree_flatten(node_or_nodes_nowrapper, is_leaf\u001b[39m=\u001b[39mis_leaf)\n\u001b[1;32m    136\u001b[0m leaves2, structure2 \u001b[39m=\u001b[39m jtu\u001b[39m.\u001b[39mtree_flatten(node_or_nodes)\n\u001b[0;32m--> 137\u001b[0m leaves2 \u001b[39m=\u001b[39m [_remove_leaf_wrapper(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m leaves2]\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m     structure1 \u001b[39m!=\u001b[39m structure2\n\u001b[1;32m    140\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(leaves1) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(leaves2)\n\u001b[1;32m    141\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39many\u001b[39m(l1 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m l2 \u001b[39mfor\u001b[39;00m l1, l2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(leaves1, leaves2))\n\u001b[1;32m    142\u001b[0m ):\n\u001b[1;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`where` must use just the PyTree structure of `pytree`. `where` must not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdepend on the leaves in `pytree`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/equinox/_tree.py:137\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m leaves1, structure1 \u001b[39m=\u001b[39m jtu\u001b[39m.\u001b[39mtree_flatten(node_or_nodes_nowrapper, is_leaf\u001b[39m=\u001b[39mis_leaf)\n\u001b[1;32m    136\u001b[0m leaves2, structure2 \u001b[39m=\u001b[39m jtu\u001b[39m.\u001b[39mtree_flatten(node_or_nodes)\n\u001b[0;32m--> 137\u001b[0m leaves2 \u001b[39m=\u001b[39m [_remove_leaf_wrapper(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m leaves2]\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m     structure1 \u001b[39m!=\u001b[39m structure2\n\u001b[1;32m    140\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(leaves1) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(leaves2)\n\u001b[1;32m    141\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39many\u001b[39m(l1 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m l2 \u001b[39mfor\u001b[39;00m l1, l2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(leaves1, leaves2))\n\u001b[1;32m    142\u001b[0m ):\n\u001b[1;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`where` must use just the PyTree structure of `pytree`. `where` must not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdepend on the leaves in `pytree`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/equinox/_tree.py:24\u001b[0m, in \u001b[0;36m_remove_leaf_wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_remove_leaf_wrapper\u001b[39m(x: _LeafWrapper) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 24\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(x) \u001b[39mis\u001b[39;00m _LeafWrapper\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mvalue\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DebugWrapper(eqx.Module):\n",
    "    name: str\n",
    "    module: eqx.Module\n",
    "    write_to: dict\n",
    "\n",
    "    def __init__(self, name, module):\n",
    "        self.name = name\n",
    "        self.module = module\n",
    "\n",
    "    def __call__(self, *args, state, **kwargs):\n",
    "        result, new_state = self.module(*args, state=state, **kwargs)\n",
    "        return result\n",
    "\n",
    "\n",
    "# short for multi-level map\n",
    "def tree_mlm(fn, tree):\n",
    "    if isinstance(tree, (int, float, bool, str, np.ndarray, jax.Array, jmp.Policy)):\n",
    "        return tree\n",
    "    leaves, treedef = jax.tree_util.tree_flatten_with_path(\n",
    "        tree, is_leaf=lambda x: x is not tree\n",
    "    )\n",
    "    if not leaves:\n",
    "        return tree\n",
    "    if any(x is tree for x in leaves):\n",
    "        return\n",
    "    leaves = [fn(path, tree_mlm(fn, leaf)) for path, leaf in leaves]\n",
    "    return jax.tree_util.tree_unflatten(treedef, leaves)\n",
    "\n",
    "\n",
    "def debugify_model(model):\n",
    "    def new_model(*args, **kwargs):\n",
    "        logs = []\n",
    "\n",
    "        def debugify(path, leaf):\n",
    "            if not isinstance(leaf, eqx.Module):\n",
    "                return leaf\n",
    "            if hasattr(leaf, \"__call__\"):\n",
    "                call = leaf.__call__\n",
    "                if hasattr(call, \"_orig_fn\"):\n",
    "                    leaf = eqx.tree_at(lambda l: l.__call__, leaf, call._orig_fn)\n",
    "                call_ = add_state(call)\n",
    "                call_._orig_fn = call\n",
    "                call = call_\n",
    "                call.store = lambda state, output: (\n",
    "                    output,\n",
    "                    {\"\".join(map(str, path)): output, **state},\n",
    "                )\n",
    "                print(leaf)\n",
    "                eqx.tree_at(lambda l: l.__call__, leaf, call)\n",
    "\n",
    "        return tree_mlm(debugify, model)(*args, **kwargs), dict(logs)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "llama_debug = jax.vmap(debugify_model(llama))\n",
    "shmoogle_smi()\n",
    "\n",
    "with mesh:\n",
    "    input_ids = [\n",
    "        tokenizer.encode(x, bos=True, eos=False)\n",
    "        for x in [\"Hello world\", \"This is a test\"]\n",
    "    ]\n",
    "    input_ids = [x + [0] * (128 - len(x)) for x in input_ids]\n",
    "    ids = jnp.asarray(input_ids)\n",
    "    ids = jax.device_put(ids, NamedSharding(mesh, spec=PartitionSpec(\"dp\", None)))\n",
    "    result = llama_debug(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "reference_llama = transformers.LlamaModel.from_pretrained(\"models/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module called: embed_tokens <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Input: torch.Size([2, 128]) torch.int64 tensor([[    1, 15043,  3186,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
      "           2.5787e-03, -3.9368e-\n",
      "Module called: layers.0.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
      "           2.5787e-03, -3.9368e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Module called: layers.0.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 1.1561e-01, -4.1347e-01,  7.6590e-01,  ..., -1.1394e-01,\n",
      "           6.2359e-01,  1.6259e-\n",
      "Module called: layers.0.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[-4.5098e-01, -1.6371e-02,  4.9824e-02,  ...,  7.0969e-01,\n",
      "           2.1830e-01,  2.7735e-\n",
      "Module called: layers.0.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "Input: torch.Size([2, 128, 4096]) torch.float32 tensor([[[ 6.7356e-03, -5.5986e-03,  9.5712e-05,  ..., -1.0382e-02,\n",
      "           3.4557e-03, -2.9162e-\n",
      "Output: torch.Size([2, 128, 4096]) torch.float32 tensor([[[-6.0133e-03, -6.2898e-03,  5.6460e-03,  ...,  1.5145e-03,\n",
      "          -6.9511e-04, -1.4681e-\n",
      "Module called: layers.0.self_attn.rotary_emb <class 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding'>\n",
      "Input: torch.Size([2, 32, 128, 128]) torch.float32 tensor([[[[-6.0133e-03, -6.2898e-03,  5.6460e-03,  ...,  1.4103e-03,\n",
      "            2.0182e-02, -5.3975\n",
      "Output: torch.Size([128, 128]) torch.float32 tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.6479,  0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/micrlhf/main.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     module\u001b[39m.\u001b[39mregister_forward_hook(make_torch_hook(name))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(input_ids)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B35.204.89.203/home/neverix/micrlhf/main.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m reference_llama(input_ids)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    923\u001b[0m         hidden_states,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    926\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    673\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    674\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    675\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    676\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    677\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    678\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    679\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:377\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 377\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39mkv_seq_len)\n\u001b[1;32m    378\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def make_torch_hook(name):\n",
    "    def torch_hook(module, input, output):\n",
    "        print(f\"Module called: {name} {module.__class__}\")\n",
    "        for arg in input:\n",
    "            if isinstance(arg, torch.Tensor):\n",
    "                print(\"Input:\", arg.shape, arg.dtype, str(arg)[:100])\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            print(\"Output:\", output.shape, output.dtype, str(output)[:100])\n",
    "        return output\n",
    "\n",
    "    return torch_hook\n",
    "\n",
    "\n",
    "for name, module in reference_llama.named_modules():\n",
    "    module._forward_hooks.clear()\n",
    "    module.register_forward_hook(make_torch_hook(name))\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "reference_llama(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
